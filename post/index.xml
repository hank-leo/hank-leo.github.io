<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Hank&#39;s Blog</title>
    <link>https://hank-leo.github.io/post/</link>
    <description>Recent content in Posts on Hank&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 11 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://hank-leo.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Python练习</title>
      <link>https://hank-leo.github.io/post/python/python%E7%BB%83%E4%B9%A0/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/python/python%E7%BB%83%E4%B9%A0/</guid>
      <description>实例001：数字组合 题目： 有四个数字：1、2、3、4，能组成多少个互不相同且无重复数字的三位数？各是多少？ 程序分析： 遍历全部可能，把有重复的剃掉。 total=0 for i in range(1,5): for j in range(1,5): for k in range(1,5): if ((i!=j)and(j!=k)and(k!=i)): print(i,j,k) total+=1 print(total) 简便方法： 用itertools中的permutations即可。 import itertools sum2=0 a=[1,2,3,4] for i in itertools.permutations(a,3): print(i) sum2+=1 print(sum2) 实例002：</description>
    </item>
    
    <item>
      <title>Python面试题</title>
      <link>https://hank-leo.github.io/post/python/Python%E9%9D%A2%E8%AF%95%E9%A2%98/</link>
      <pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/python/Python%E9%9D%A2%E8%AF%95%E9%A2%98/</guid>
      <description>Python语言特性 1 Python的函数参数传递 看两个例子: a = 1 def fun(a): a = 2 fun(a) print a # 1 a = [] def fun(a): a.append(1) fun(a) print a # [1] 所有的变量都可以理解是内存中一个对象的“引用”，或者，也可以看似c中void*的感觉。 通过id来看引用a的内存地址可以比较理解： a = 1 def fun(a): print &amp;quot;func_in&amp;quot;,id(a) # func_in 41322472 a = 2 print &amp;quot;re-point&amp;quot;,id(a), id(2) # re-point 41322448 41322448 print &amp;quot;func_out&amp;quot;,id(a),</description>
    </item>
    
    <item>
      <title>爬虫面试题</title>
      <link>https://hank-leo.github.io/post/spider/%E7%88%AC%E8%99%AB%E9%9D%A2%E8%AF%95%E9%A2%98/</link>
      <pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/spider/%E7%88%AC%E8%99%AB%E9%9D%A2%E8%AF%95%E9%A2%98/</guid>
      <description>基础 http协议, tcp协议 top 命令 Linux/Mac 下虚拟内存 (Swap) 用过的linux命令，具体细节比如crontab命令报错可能有哪些原因 线程、进程、协程 Async 相关、事件驱动相关 阻塞、非阻塞 Python GIL 布隆过滤器原理：如何实现、一般要几次哈希函数 MySQL的索引底层 实战 抓取天猫超市上某些商品配送省份信息 (主要需</description>
    </item>
    
    <item>
      <title>使用hexo搭建个人博客</title>
      <link>https://hank-leo.github.io/post/blog/%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</link>
      <pubDate>Fri, 06 Dec 2019 21:23:32 +0800</pubDate>
      
      <guid>https://hank-leo.github.io/post/blog/%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</guid>
      <description>新建项目 首先在 GitHub 新建一个仓库Repository, 名称为username.githu.io, 注意这个名比较特殊, 必须要是github.io为后缀结尾的。要与用户名一致，比如ihankleo的 GitHub 用户名就叫ihankleo.github.io, 新建完成之后就可以进行后续操作了。 在此之</description>
    </item>
    
    <item>
      <title>Scrapy框架:Scrapy&#43;MongoDB实战:抓取并保存IT之家博客新闻</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Scrapy&#43;MongoDB%E5%AE%9E%E6%88%98%E6%8A%93%E5%8F%96%E5%B9%B6%E4%BF%9D%E5%AD%98IT%E4%B9%8B%E5%AE%B6%E5%8D%9A%E5%AE%A2%E6%96%B0%E9%97%BB/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Scrapy&#43;MongoDB%E5%AE%9E%E6%88%98%E6%8A%93%E5%8F%96%E5%B9%B6%E4%BF%9D%E5%AD%98IT%E4%B9%8B%E5%AE%B6%E5%8D%9A%E5%AE%A2%E6%96%B0%E9%97%BB/</guid>
      <description>创建项目 scrapy startproject ithome cd ithome scrapy genspider -t crawl news ithome.com 编写items.py文件 # -*- coding: utf-8 -*- # Define here the models for your scraped items # # See documentation in: # https://doc.scrapy.org/en/latest/topics/items.html import scrapy class IthomeItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() #文章标题 title = scrapy.Field() #文章url url = scrapy.Field() #来源 source = scrapy.Field() #来源url source_url = scrapy.Field() #发布日期 release_大特＝ scrapy.Field() #作者 author = scrapy.Field() #关键词 key_words = scrapy.Field() 编写爬虫文件 news.py # -*- coding: utf-8 -*- import</description>
    </item>
    
    <item>
      <title>Scrapy框架:通用爬虫</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB/</guid>
      <description>通用爬虫之CrawlSpider 步骤01: 创建爬虫项目 scrapy startproject quotes 步骤02: 创建爬虫模版 scrapy genspider -t quotes quotes.toscrape.com 步骤03: 配置爬虫文件quotes.py import scrapy from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors import LinkExtractor class Quotes(CrawlSpider): # 爬虫名称 name = &amp;quot;get_quotes&amp;quot; allow_domain = [&#39;quotes.toscrape.com&#39;] start_urls = [&#39;http://quotes.toscrape.com/&#39;] # 设定规则 rules = ( # 对于quotes内容页URL，调用parse_quotes处理， # 并以此规则</description>
    </item>
    
    <item>
      <title>使用Airtest进行App爬虫</title>
      <link>https://hank-leo.github.io/post/app/%E4%BD%BF%E7%94%A8Airtest%E8%BF%9B%E8%A1%8CApp%E7%88%AC%E8%99%AB/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/app/%E4%BD%BF%E7%94%A8Airtest%E8%BF%9B%E8%A1%8CApp%E7%88%AC%E8%99%AB/</guid>
      <description>前言 Airtest是网易开发的手机UI戒面自动化测试工具， 安装 从Airtest官网 : https://airtest.netease.com 下载Airtest，然后像安装普通软件一样安装即可。 手机连接 启动Airtest以后，把Android手机连接到电脑上，点击下图方框中的refresh ADB 然后点击connect按钮 开始爬虫 以某品牌咖</description>
    </item>
    
    <item>
      <title>存储数据之excel</title>
      <link>https://hank-leo.github.io/post/spider/%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E4%B9%8Bexcel/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/spider/%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E4%B9%8Bexcel/</guid>
      <description>第一种方式是使用pandas库 def save(data): data = pd.DataFrame(data) data.to_excel(index=False) 第二种方式是使用openpyxl库 import openpyxl def save(title, head, data): outwb = openpyxl.Workbook() outws = outwb.active outws.title=title for h in range(len(head)): outws.cell(1,h+1).value=head[h] for row in data: outws.append(row) outwb.save(&amp;quot;path&amp;quot;) 第三种方式是使用xlwt库 def save(title, head, data): workbook = xlwt.Workbook(encoding=&#39;utf8&#39;) sheet = workbook.add_sheet(title, cell_overwrite_ok=True) for h in range(len(head)): sheet.write(0, h, head[h]) i = 1 for list in data: j = 0 for data in list: sheet.write(i, j, data) j += 1 i += 1 workbook.save(path)</description>
    </item>
    
    <item>
      <title>解析与提取数据之re</title>
      <link>https://hank-leo.github.io/post/spider/%E8%A7%A3%E6%9E%90%E4%B8%8E%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B9%8Bre/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/spider/%E8%A7%A3%E6%9E%90%E4%B8%8E%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B9%8Bre/</guid>
      <description>对比xpath, re对文本结构的数据处理更加灵活 #导入包 import re def get_data(response): #提取内容 title = re.findall(&amp;quot;^&amp;lt;span&amp;gt;(.*?)&amp;lt;/span&amp;gt;&amp;quot;,response, re.S)</description>
    </item>
    
    <item>
      <title>解析与提取数据之xpath</title>
      <link>https://hank-leo.github.io/post/spider/%E8%A7%A3%E6%9E%90%E4%B8%8E%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B9%8Bxpath/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/spider/%E8%A7%A3%E6%9E%90%E4%B8%8E%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B9%8Bxpath/</guid>
      <description>日常工作主要使用xpath进行数据的解析和提取，归于xpath有着强大的功能，适应大多数的网页结构。 #导入包 from lxml import etree def get_data(response): html=etree.HTML(response) #提取属性值 url=html.xpath(&amp;quot;//div[@href]&amp;quot;) #提取文本 title=html.xpath(&amp;quot;//div[@class=&#39;title&#39;]/text()&amp;quot;)</description>
    </item>
    
    <item>
      <title>MongoDB的实际应用</title>
      <link>https://hank-leo.github.io/post/sql/MongoDB%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/sql/MongoDB%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</guid>
      <description>MongoDB基本操作 1.数据库操作 使用use DATABASE_NAME创建数据库 use maitian 如果数据库不存在，就创建数据库，否则切换到指定的数据库 使用show dbs查看所有数据库 show dbs 使用db.dropDatabase()删除数据库 use maitian db.dropDatabase() 2.集合操作 在maitian数据库中创建名为zuf</description>
    </item>
    
    <item>
      <title>Xpath的实际应用</title>
      <link>https://hank-leo.github.io/post/python/xpath%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/python/xpath%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</guid>
      <description>xpath中使用contains xpath(span[contanins(@class, &#39;xxx&#39;)]) Xpath如何选择不包含某一个属性的节点? 这里可以用到 not 例如排除一个属性的节点可以使用 //tbody/tr[not(@class)] 排除一个或者两个属性可以使用 //tbody/tr[not(@class or @id)] xpath按序选择 有时候我们在选择的时候可能某些属性同时匹配了多个节点，但是我们只想要其中的某个节点，如第二个节点，或者最</description>
    </item>
    
    <item>
      <title>使用hugo搭建个人博客</title>
      <link>https://hank-leo.github.io/post/blog/%E4%BD%BF%E7%94%A8hugo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/blog/%E4%BD%BF%E7%94%A8hugo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</guid>
      <description>安装 在这个页面下载安装https://github.com/gohugoio/hugo/releases 下载完成后解压，将解压出来的可执行文件，放到自定义目录下，并将选择的路径放入环境变量path中 初始化 下面开始存放我们的博客，在选好的路径下执行 hugo new site ihankleo.github.io 命令执行完会创建一个名为m</description>
    </item>
    
    <item>
      <title>列表的实际应用</title>
      <link>https://hank-leo.github.io/post/python/%E5%88%97%E8%A1%A8%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/python/%E5%88%97%E8%A1%A8%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</guid>
      <description>列表合并保留最大长度 import itertools w, x, y, z = [], [1], [2, 3], [4, 5, 6] longest_wxyz = itertools.zip_longest(w, x, y, z) print(list(longest_wxyz)) 结果: [(None, 1, 2, 4), (None, None, 3, 5), (None, None, None, 6)] 列表元素替换 lst = [&#39;1&#39;,&#39;2&#39;,&#39;3&#39;] rep = [&#39;4&#39; if x == &#39;2&#39; else x for x in lst] print(rep) 结果: [&amp;lsquo;1&amp;rsquo;, &amp;lsquo;4&amp;rsquo;, &amp;lsquo;3&amp;rsquo;] 列表进行去重操作 一般的去重操作后是出现乱序的情况 t=[&#39;8&#39;,&#39;7&#39;,&#39;2&#39;,&#39;中国&#39;,&#39;China&#39;,&#39;中国&#39;,&#39;1&#39;,&#39;4&#39;]</description>
    </item>
    
    <item>
      <title>字典的实际应用</title>
      <link>https://hank-leo.github.io/post/python/%E5%AD%97%E5%85%B8%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/python/%E5%AD%97%E5%85%B8%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</guid>
      <description>1.字典排序 利用key排序 d = {&#39;d1&#39;:2, &#39;d2&#39;:4, &#39;d4&#39;:1,&#39;d3&#39;:3,} for k in sorted(d): print(k,d[k]) d1 2 d2 4 d3 3 d4 1 利用value排序：getitem d = {&#39;d1&#39;:2, &#39;d2&#39;:4, &#39;d4&#39;:1,&#39;d3&#39;:3,} for k in sorted(d,key=d.__getitem__): print(k,d[k]) d4 1 d1 2 d3 3 d2 4 反序: reverse=True d = {&#39;d1&#39;:2, &#39;d2&#39;:4, &#39;d4&#39;:1,&#39;d3&#39;:3,} for k in sorted(d,key=d.__getitem__,reverse=True): print(k,d[k]) d2 4 d3 3 d1 2 d4 1 对dict_items进行排序 d = {&#39;d1&#39;:2, &#39;d2&#39;:4, &#39;d4&#39;:1,&#39;d3&#39;:3,} res = sorted(d.items(),key=lambda d:d[1],reverse=True) print(res) [(&amp;lsquo;d2&amp;rsquo;, 4), (&amp;lsquo;d3&amp;rsquo;, 3), (&amp;lsquo;d1&amp;rsquo;, 2), (&amp;lsquo;d4&amp;rsquo;, 1)] 2.两个字典（dict）合并 dict1 = { &amp;quot;name&amp;quot;:&amp;quot;owen&amp;quot;, &amp;quot;age&amp;quot;:</description>
    </item>
    
    <item>
      <title>字符串的实际应用</title>
      <link>https://hank-leo.github.io/post/python/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/python/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</guid>
      <description>1.判断字符串是否为小数 try: lat = float(location.split(&#39;,&#39;)[1]) lon = float(location.split(&#39;,&#39;)[0]) except ValueError: print(&#39;no number&#39;) 2.用split对字符串进行分割 str=&#39;storeId=ff8080816277aa0a0162845d48e3012b&amp;amp;appid=wxe37b2e703155ed41&amp;amp;transId=wxe37b2e703155ed412019-05-09%2010%3A28%3A15&amp;amp;sign=dc8fac903b03556247659e1b548bccce&amp;amp;timestamp=2019-05-09%2010%3A28%3A15&amp;amp;memberId=ff8080816a889e71016a9a68a6c55a37&amp;amp;cliqueId=-1&amp;amp;cliqueMemberId=-1&amp;amp;useClique=0&amp;amp;enterpriseId=ff808081624e60f601625c50a30900ce&amp;amp;unionid=oLWn80pR0DtSJXfnO_1O4ZOzfvAE&amp;amp;openid=oZe8D5gmPcPANw4kNNcG8mlAW1mI&amp;amp;launchOptions=%7B%22path%22%3A%22%2Fpages%2Fmall%2Fmall-index%2Fmall-index%22%2C%22query%22%3A%7B%7D%2C%22scene%22%3A1102%2C%22referrerInfo%22%3A%7B%22appId%22%3A%22wx97e5123eb6041454%22%7D%7D&#39; str2=str.split(&#39;&amp;amp;&#39;) for i in str2: print(&#39;&amp;quot;&#39;+i.split(&#39;=&#39;)[0]+&#39;&amp;quot;:&amp;quot;&#39;+i.split(&#39;=&#39;)[1]+&#39;&amp;quot;,&#39;) 结果如下： &amp;ldquo;storeId&amp;rdquo;:&amp;ldquo;ff8080816277aa0a0162845d48e3012b&amp;rdquo;, &amp;ldquo;appid&amp;rdquo;:&amp;ldquo;wxe37b2e703155ed41&amp;rdquo;, &amp;ldquo;transId&amp;rdquo;:&amp;ldquo;wxe37b2e703155ed412019-05-09%2010%3A28%3A15&amp;rdquo;, &amp;ldquo;sign&amp;rdquo;:&amp;ldquo;dc8fac903b03556247659e1b548bccce&amp;rdquo;, &amp;ldquo;timestamp&amp;rdquo;:&amp;ldquo;2019-05-09%2010%3A28%3A15&amp;rdquo;, &amp;ldquo;memberId&amp;rdquo;:&amp;ldquo;ff8080816a889e71016a9a68a6c55a37&amp;rdquo;, &amp;ldquo;cliqueId&amp;rdquo;:&amp;quot;-1&amp;rdquo;, &amp;ldquo;cliqueMemberId&amp;rdquo;:&amp;quot;-1&amp;rdquo;, &amp;ldquo;useClique&amp;rdquo;:&amp;ldquo;0&amp;rdquo;, &amp;ldquo;enterpriseId&amp;rdquo;:&amp;ldquo;ff808081624e60f601625c50a30900ce&amp;rdquo;, &amp;ldquo;unionid&amp;rdquo;:&amp;ldquo;oLWn80pR0DtSJXfnO_1O4ZOzfvAE&amp;rdquo;, &amp;ldquo;openid&amp;rdquo;:&amp;ldquo;oZe8D5gmPcPANw4kNNcG8mlAW1mI&amp;rdquo;, &amp;ldquo;launchOptions&amp;rdquo;:&amp;quot;%7B%22path%22%3A%22%2Fpages%2Fmall%2Fmall-index%2Fmall-index%22%2C%22query%22%3A%7B%7D%2C%22scene%22%3A1102%2C%22referrerInfo%22%3A%7B%22appId%22%3A%22wx97e5123eb6041454%22%7D%7D&amp;rdquo;, 3.python检测字符串乱码 import chardet f=open(&#39;test.txt&#39;,&#39;rb&#39;) f_read=f.read() f_charInfo=chardet.detect(f_read) print(f_charInfo) # f_charInfo的输出是这样的的一个字典{&#39;confidence&#39;: 0.99, &#39;encoding&#39;: &#39;utf-8&#39;} 4.将逗号分隔的字符串转换为P</description>
    </item>
    
    <item>
      <title>正则的实际应用</title>
      <link>https://hank-leo.github.io/post/python/%E6%AD%A3%E5%88%99%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/python/%E6%AD%A3%E5%88%99%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</guid>
      <description>grep hank@hank:~$ cat grep.txt ooxx121212121ooxx ooxx 12121212 oox 12121212 1212 ooxx 1212 oo3xx oo4xx ooWxx oomxx $ooxx oo1234xx ooxyzxx hank@hank:~$ grep &amp;quot;ooxx&amp;quot; grep.txt ooxx121212121ooxx ooxx 12121212 1212 ooxx 1212 $ooxx hank@hank:~$ grep &amp;quot;[34]&amp;quot; grep.txt oo3xx oo4xx oo1234xx hank@hank:~$ grep &amp;quot;[0-9]\{4\}&amp;quot; grep.txt ooxx121212121ooxx ooxx 12121212 oox 12121212 1212 ooxx 1212 oo1234xx hank@hank:~$ grep -E &amp;quot;[0-9]{4}&amp;quot; grep.txt ooxx121212121ooxx ooxx 12121212 oox 12121212 1212 ooxx 1212 oo1234xx hank@hank:~$ grep &amp;quot;\&amp;lt;ooxx\&amp;gt;&amp;quot; grep.txt ooxx 12121212 1212 ooxx 1212 $ooxx hank@hank:~$ grep &amp;quot;[^0-9][0-9]\{4\}[^0-9]&amp;quot; grep.txt oo1234xx hank@hank:~$ grep &amp;quot;\(^[0-9]\|[^0-9][0-9]\)[0-9]\{2\}\([0-9]$\|[0-9][^0-9]\)&amp;quot; grep.txt 1212 ooxx 1212 oo1234xx hank@hank:~$ grep -E &amp;quot;(^[0-9]|[^0-9][0-9])[0-9]{2}([0-9]$|[0-9][^0-9])&amp;quot; grep.txt 1212 ooxx 1212 oo1234xx</description>
    </item>
    
    <item>
      <title>MySQL的实际应用</title>
      <link>https://hank-leo.github.io/post/sql/MySQL%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/sql/MySQL%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</guid>
      <description>创建触发器语法 create trigger tgName after/before insert/delete/update on tableName for each row sql; -- 触发语句 删除触发器 drop trigger tgName; 索引 提高查询速度,但是降低了增删改的速度, 所以使用索引时,要综合考虑. 索引不是越多越好,一般我们在常出现于条件表达式中的列加索引. 值越分散的列，索引的效果越好 索引类型 |索引|解释 :-:|:-: primary key|主键索引 index|普通索引</description>
    </item>
    
    <item>
      <title>Pandas异常值处理</title>
      <link>https://hank-leo.github.io/post/analysis/Pandas%E5%BC%82%E5%B8%B8%E5%80%BC%E5%A4%84%E7%90%86/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/analysis/Pandas%E5%BC%82%E5%B8%B8%E5%80%BC%E5%A4%84%E7%90%86/</guid>
      <description>导包 import pandas as pd 生成异常数据 df=pd.DataFrame({&#39;col1&#39;:[1,120,3,5,2,12,13],&#39;col2&#39;:[12,17,31,53,22,32,43]}) #打印 print(df) col1 col2 0 1 12 1 120 17 2 3 31 3 5 53 4 2 22 5 12 32 6 13 43 df_zscore=df.copy() #复制一个用来存储Z-score得分的数据框 cols=df.columns for col in cols: df_col=df[col] z_score=(df_col - df_col.mean()) / df_col.std() #计算每列的Z-score得分 df_zscore[col] = z_score.abs() &amp;gt; 2.2 #判断Z-score得分是否大于2.2,如果是则为True,否则为False #打印，为Tru</description>
    </item>
    
    <item>
      <title>Pandas缺失值处理</title>
      <link>https://hank-leo.github.io/post/analysis/Pandas%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/analysis/Pandas%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86/</guid>
      <description>导入库 import pandas as pd import numpy as np from sklearn.preprocessing import Imputer 生成缺失数据 df=pd.DataFrame(np.random.randn(6,4),columns=[&#39;col1&#39;,&#39;col2&#39;,&#39;col3&#39;,&#39;col4&#39;]) df.iloc[1:2,1] = np.nan #增加缺失值 df.iloc[4,3] = np.nan #增加缺失值 #打印输出 print(df) col1 col2 col3 col4 0 -0.977511 -0.566332 -0.529934 1.489695 1 -0.491128 NaN -0.811174 -1.102717 2 0.385777 -0.638822 0.325953 -0.240780 3 0.938351 -0.746889 0.375200 -0.715265 4 1.103418 0.238959 -0.459114 NaN 5 1.002177 0.448844 -0.584634 -1.038151 查看缺失值位置 nan_all=df.isnull() #打印 print(nan_all) col1 col2 col3 col4 0 False False False False 1 False True False False 2 False False False False 3 False False False False 4 False False False True 5 False False False False #获取含有NA的列 nan_col1=df.isnull().any() #打印 print(nan_col1)</description>
    </item>
    
    <item>
      <title>Pandas重复值处理</title>
      <link>https://hank-leo.github.io/post/analysis/Pandas%E9%87%8D%E5%A4%8D%E5%80%BC%E5%A4%84%E7%90%86/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/analysis/Pandas%E9%87%8D%E5%A4%8D%E5%80%BC%E5%A4%84%E7%90%86/</guid>
      <description>导包 import pandas as pd 生成数据 data1,data2,data3,data4=[&#39;a&#39;,3],[&#39;b&#39;,2],[&#39;a&#39;,3],[&#39;c&#39;,2] df=pd.DataFrame([data1,data2,data3,data4],columns=[&#39;col1&#39;,&#39;col2&#39;]) print(df) col1 col2 0 a 3 1 b 2 2 a 3 3 c 2 判断数据 isDuplicated=df.duplicated() #判断重复数据记录 print(isDuplicated) 0 False 1 False 2 True 3 False dtype: bool 删除重复的数据 print(df.drop_duplicates()) #删除所有列值相同的记录，index为2的记录行被删除 col1 col2 0 a 3 1 b 2 3 c 2 #删除col1列值相同的记录，index为2的记录行被删除 print(df.drop_duplicates([&#39;col1&#39;])) col1 col2 0 a 3 1 b 2 3 c 2 #</description>
    </item>
    
    <item>
      <title>Scrapy框架 基本命令</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/</guid>
      <description>创建爬虫项目 scrapy startproject [项目名称] 创建爬虫文件 scrapy genspider +文件名+网址 运行(crawl) scrapy crawl 爬虫名称 # -o output 输出数据到文件 scrapy crawl [爬虫名称] -o zufang.json scrapy crawl [爬虫名称] -o zufang.csv check检查错误 scrapy check list返回项目所有spider scrapy list view 存储、打开网页 scrapy view http://www.baidu.com scrapy shell, 进入终端 scrapy shell https://www.baidu.com scrapy runspider scrapy runspider zufang_spider.py</description>
    </item>
    
    <item>
      <title>Scrapy框架 登录网站</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%99%BB%E5%BD%95%E7%BD%91%E7%AB%99/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%99%BB%E5%BD%95%E7%BD%91%E7%AB%99/</guid>
      <description>使用cookies登录网站 import scrapy class LoginSpider(scrapy.Spider): name = &#39;login&#39; allowed_domains = [&#39;xxx.com&#39;] start_urls = [&#39;https://www.xxx.com/xx/&#39;] cookies = &amp;quot;&amp;quot; def start_requests(self): for url in self.start_urls: yield scrapy.Request(url, cookies=self.cookies, callback=self.parse) def parse(self, response): with open(&amp;quot;01login.html&amp;quot;, &amp;quot;wb&amp;quot;) as f: f.write(response.body) 发送post请求登录, 要手动解析网页获取登录参数 import scrapy class LoginSpider(scrapy.Spider): name=&#39;login_code&#39; allowed_domains = [&#39;xxx.com&#39;] #1. 登录页面 start_urls = [&#39;https://www.xxx.com/login/&#39;] def parse(self, response): #2. 代码登录 login_url=&#39;https://www.xxx.com/login&#39; formdata={ &amp;quot;username&amp;quot;:&amp;quot;xxx&amp;quot;, &amp;quot;pwd&amp;quot;:&amp;quot;xxx&amp;quot;, &amp;quot;formhash&amp;quot;:response.xpath(&amp;quot;//input[@id=&#39;formhash&#39;]/@value&amp;quot;).extract_first(), &amp;quot;backurl&amp;quot;:response.xpath(&amp;quot;//input[@id=&#39;backurl&#39;]/@value&amp;quot;).extract_first() } #3. 发送登录请求post yield scrapy.FormRequest(login_url, formdata=formdata, callback=self.parse_login) def parse_login(self, response): #4.访问目标页面 member_url=&amp;quot;https://www.xxx.com/member&amp;quot; yield scrapy.Request(member_url, callback=self.parse_member) def parse_member(self, response): with open(&amp;quot;02login.html&amp;quot;,&#39;wb&#39;) as</description>
    </item>
    
    <item>
      <title>Scrapy框架:Request回调函数</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Request%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Request%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0/</guid>
      <description>Request回调函数 def parse_page1(self, response): return scrapy.Request(&amp;quot;http://www.example.com/some_page.html&amp;quot;, callback=self.parse_page2) def parse_page2(self, response): # this would log http://www.example.com/some_page.html self.logger.info(&amp;quot;Visited %s&amp;quot;, response.url) 传递参数 def parse_page1(self, response): item = MyItem() item[&#39;name&#39;] = response.css(&#39;.name::text&#39;).extract_first() request = scrapy.Request(&amp;quot;http://www.example.com/some_page.html&amp;quot;, callback=self.parse_page2) request.meta[&#39;item&#39;] = item yield request def parse_page2(self, response): item = response.meta[&#39;item&#39;] item[&#39;age&#39;] = response.css(&#39;.age::text&#39;).extract_first() yield item</description>
    </item>
    
    <item>
      <title>Scrapy框架:Settings.py</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-settings-py/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-settings-py/</guid>
      <description>#Scrapy项目名字 BOT_NAME = &#39;segmentfault&#39; #Scrapy搜索spider的模块列表 SPIDER_MODULES = [&#39;segmentfault.spiders&#39;] #使用爬虫创建命令genspider创建爬虫时生成的模块 NEWSPIDER_MODULE = &#39;segmentfault.spiders&#39; #默认的USER_AGENT, 使用BOT_NAME配置生成，建议覆盖 #USER_AGNET = &#39;segmentfault (+http://www.yourdomain.com)&#39; #如果启用，Scrapy则会遵守网站Rebots.txt协议，建议设</description>
    </item>
    
    <item>
      <title>Scrapy框架:入门案例</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/</guid>
      <description>创建项目: scrappy start project maitian 明确要抓取的字段items.py import scrapy class MaitianItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() title = scrapy.Field() price = scrapy.Field() area = scrapy.Field() district = scrapy.Field() 在spider目录下创建爬虫文件: zufang_spider.py 2.1 创建一个类，并继承scrapy的一个子类: scrapy.Spider 2.2 自定义爬取名, name=&amp;quot;&amp;quot; 后面运行框架需要用到； 2.3 定义爬取目标网址 2.4 定义scrapy的方法 下面是简</description>
    </item>
    
    <item>
      <title>Scrapy框架:异常处理</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/</guid>
      <description>import scrapy from scrapy.spidermiddlewares.httperror import HttpError from twisted.internet.error import DNSLookupError from twisted.internet.error import TimeoutError, TCPTimedOutError class ErrbackSpider(scrapy.Spider): name = &amp;quot;errback_example&amp;quot; start_urls = [ &amp;quot;http://www.httpbin.org/&amp;quot;, # 正常HTTP 200返回 &amp;quot;http://www.httpbin.org/status/404&amp;quot;, # 404 Not found error &amp;quot;http://www.httpbin.org/status/500&amp;quot;, # 500服务器错误 &amp;quot;http://www.httpbin.org:12345/&amp;quot;, # 超时无响应错误 &amp;quot;http://www.httphttpbinbin.org/&amp;quot;, # DNS 错误 ] def start_requests(self): for u in self.start_urls: yield scrapy.Request(u, callback=self.parse_httpbin, errback=self.errback_httpbin, dont_filter=True) def parse_httpbin(self, response): self.logger.info(&#39;Got successful response from {}&#39;.format(response.url)) # 其他处理. def errback_httpbin(self, failure): # 日志记录所有的异常信息 self.logger.error(repr(failure)) # 假设我们需要对指定的异常类型做处理， # 我们需要判断异常的类型 if</description>
    </item>
    
    <item>
      <title>Scrapy框架:抓取猫眼电影TOP100</title>
      <link>https://hank-leo.github.io/post/scrapy/scrapy%E6%A1%86%E6%9E%B6-%E6%8A%93%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1TOP100/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/scrapy%E6%A1%86%E6%9E%B6-%E6%8A%93%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1TOP100/</guid>
      <description>需求： 抓取的数据为电影名称、主演、上映日期、评分。将抓取的数据保存到maoyantop100.json文件，并将文件作为附件通过邮件发送给接收人。 创建项目 scrapy startproject maoyan scrapy genspider -t crawl top100 maoyan.com 编写items.py # -*- coding: utf-8 -*- import scrapy class MaoyanItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 电影名称 name = scrapy.Field() # 主演 actors = scrapy.Field() # 上映时间 releasetime = scrapy.Field()</description>
    </item>
    
    <item>
      <title>Scrapy框架:爬取链家二手房信息</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%88%AC%E5%8F%96%E9%93%BE%E5%AE%B6%E4%BA%8C%E6%89%8B%E6%88%BF%E4%BF%A1%E6%81%AF/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%88%AC%E5%8F%96%E9%93%BE%E5%AE%B6%E4%BA%8C%E6%89%8B%E6%88%BF%E4%BF%A1%E6%81%AF/</guid>
      <description>创建爬虫项目 scrapy startproject lianjiahouse 创建爬虫文件 scrapy genspider -t craw house lianjia.com 编写items.py文件 # -*- coding: utf-8 -*- # Define here the models for your scraped items # # See documentation in: # https://doc.scrapy.org/en/latest/topics/items.html import scrapy class LianjiahouseItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 发布信息名称 house_name = scrapy.Field() # 小区名称 community_name = scrapy.Field() # 所在区域 # location = scrapy.Field() # 链家编号 house_record = scrapy.Field() # 总售价 total_amount = scrapy.Field() # 单价 unit_price = scrapy.Field() # 房屋基本信息 # 建筑面积 area_total = scrapy.Field() # 套内面积 area_use</description>
    </item>
    
    <item>
      <title>Scrapy框架:统计数据收集</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/</guid>
      <description>通过stats属性来使用数据收集器 class ExtensionThatAccessStats(object): def __init__(self, stats): self.stats = stats @classmethod def from_crawler(cls, crawler): return cls(crawler.stats) 设置数据 stats.set_value(&#39;hostname&#39;, socket.gethostname()) 增加数据值 stats.inc_value(&#39;pages_crawled&#39;) 当新的值比原来的值大时设置数据 stats.max_value(&#39;max_items_scraoed&#39;, value) 当新的值比原来的值小时设置数据 stats.min_value(&#39;min_free_memory_percent&#39;, value) 获取数据 stats.get_value(&#39;pages_crawled&#39;) 获取所有数据 stats.get_stats() 示例：统计名人名言网站(http://quotes.toscrape.com/)标签为love的名言数</description>
    </item>
    
  </channel>
</rss>