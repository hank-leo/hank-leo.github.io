<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Hank&#39;s Blog</title>
    <link>https://hank-leo.github.io/post/</link>
    <description>Recent content in Posts on Hank&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 27 Aug 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://hank-leo.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>LeetCode SQL刷题记录</title>
      <link>https://hank-leo.github.io/post/sql/LeetCode-SQL%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/sql/LeetCode-SQL%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/</guid>
      <description>175.【简单】组合两个表 1 select p.FirstName,p.LastName,a.City,a.State from person p left join address a on p.personid=a.personid; 176.【简单】第二高的薪水 1 select ifnull((select distinct(Salary) from Employee order by Salary desc limit 1,1),null) as SecondHighestSalary; 177.【中等】第N高的薪水 1 2 3 4 5 6 7 8 9 10 11 CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INT BEGIN set n=n-1; RETURN ( # Write your MySQL query statement below. select distinct Salary as getNthHighestSalary from Employee order by Salary DESC limit N,1 ); END 181.【简单】超过经理收入的员工 1 2 select e1.Name as Employee from Employee as e1,Employee as e2</description>
    </item>
    
    <item>
      <title>爬取某著名快餐巨头门店数据实战</title>
      <link>https://hank-leo.github.io/post/spider/%E7%88%AC%E5%8F%96%E6%9F%90%E8%91%97%E5%90%8D%E5%BF%AB%E9%A4%90%E5%B7%A8%E5%A4%B4%E9%97%A8%E5%BA%97%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%88%98/</link>
      <pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/spider/%E7%88%AC%E5%8F%96%E6%9F%90%E8%91%97%E5%90%8D%E5%BF%AB%E9%A4%90%E5%B7%A8%E5%A4%B4%E9%97%A8%E5%BA%97%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%88%98/</guid>
      <description>项目说明 通过python代码抓取*德基官网门店数据 爬取过程 首先查看网页结构可知，数据以json格式存储 要拿到请求参数才能访问数据，这里的参数主要是cname，也就是城市信息 解析网页信息，获取城市数据 1 2 3 4 5 6 7 8 9 10 11 def get_city(self): try: resp=session.get(url=self.base_url,headers=self.headers,verify=False).content.decode() sel=Selector(resp) all_li=sel.xpath(&amp;#34;//ul[@class=&amp;#39;shen_info&amp;#39;]/li&amp;#34;) for p in all_li: all_a=p.xpath(&amp;#34;./div[@class=&amp;#39;shen_city&amp;#39;]/a&amp;#34;) for c in all_a: city=c.xpath(&amp;#34;./text()&amp;#34;).get() except Exception: traceback.print_exc() 这里用到Selecto</description>
    </item>
    
    <item>
      <title>Aiohttp&#43;PyMongo异步案例实战</title>
      <link>https://hank-leo.github.io/post/spider/Aiohttp&#43;PyMongo%E5%BC%82%E6%AD%A5%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/spider/Aiohttp&#43;PyMongo%E5%BC%82%E6%AD%A5%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98/</guid>
      <description>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 import json import asyncio import aiohttp import logging from motor.motor_asyncio import AsyncIOMotorClient logging.basicConfig(level=logging.INFO,format=&amp;#34;%(asctime)s- %(levelname)s: %(message)s&amp;#34;) MONGO_CONNECTION_STRING = &amp;#34;mongodb://localhost:27017&amp;#34; MONGO_DB_NAME = &amp;#34;books&amp;#34; MONGO_COLLECTION_NAME = &amp;#34;books&amp;#34; client = AsyncIOMotorClient(MONGO_CONNECTION_STRING) db = client[MONGO_DB_NAME] collection = db[MONGO_COLLECTION_NAME] INDEX_URL = &amp;#34;https://dynamic5.scrape.cuiqingcai.com/api/book/?limit=18&amp;amp;offset={offset}&amp;#34; DETAIL_URL = &amp;#34;https://dynamic5.scrape.cuiqingcai.com/detail/{id}&amp;#34; PAGE_SIZE = 18 PAGE_NUMBER = 100 CONCURRENCY = 5 semaphore = asyncio.Semaphore(CONCURRENCY) session = None async def scrape_api(url): async with semaphore: try: logging.info(&amp;#34;scraping %s&amp;#34;, url) async with session.get(url) as response: return await</description>
    </item>
    
    <item>
      <title>PyMongo&#43;PyQuery案例实战</title>
      <link>https://hank-leo.github.io/post/spider/PyMongo&#43;PyQuery%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/spider/PyMongo&#43;PyQuery%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98/</guid>
      <description>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 import requests import logging import re import pymongo from pyquery import PyQuery as pq from urllib.parse import urljoin import multiprocessing logging.basicConfig(level=logging.INFO,format=&amp;#39;%(asctime)s-%(levelname)s:%(message)s&amp;#39;) BASE_URL=&amp;#39;https://static1.scrape.cuiqingcai.com&amp;#39; TOTAL_PAGE=10 MONGO_CONNECTION_STRING=&amp;#39;mongodb://localhost:27017&amp;#39; MONGO_DB_NAME=&amp;#39;movies&amp;#39; MONGO_COLLECTION_NAME=&amp;#39;movies&amp;#39; client=pymongo.MongoClient(MONGO_CONNECTION_STRING) db=client[&amp;#39;movies&amp;#39;] collection=db[&amp;#39;movies&amp;#39;] def scrape_page(url): logging.info(&amp;#39;scraping %s...&amp;#39;,url) try: response=requests.get(url) if response.status_code==200: return response.text logging.error(&amp;#39;get invalid status code %swhile scraping %s&amp;#39;,response.status_code,url) except requests.RequestException: logging.error(&amp;#34;error occurred while</description>
    </item>
    
    <item>
      <title>SQL复习</title>
      <link>https://hank-leo.github.io/post/sql/SQL%E5%A4%8D%E4%B9%A0/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/sql/SQL%E5%A4%8D%E4%B9%A0/</guid>
      <description>一:复习前的准备 1:确认你已安装wamp 2:确认你已安装ecshop,并且ecshop的数据库名为shop 二:基础知识: 1.数据库的连接 mysql -u -p -h -u 用户名 -p 密码 -h host主机 2:库级知识 2.1 显示数据库: show databases; 2.2 选择数据库: use dbname; 2.3 创建数据库: create database dbname charset utf8; 2.3 删除数据库: drop database dbname; 3: 表级操作: 3.1 显</description>
    </item>
    
    <item>
      <title>使用Airtest爬取某咖啡app数据</title>
      <link>https://hank-leo.github.io/post/app/%E4%BD%BF%E7%94%A8Airtest%E7%88%AC%E5%8F%96%E6%9F%90%E5%92%96%E5%95%A1app%E6%95%B0%E6%8D%AEv1/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/app/%E4%BD%BF%E7%94%A8Airtest%E7%88%AC%E5%8F%96%E6%9F%90%E5%92%96%E5%95%A1app%E6%95%B0%E6%8D%AEv1/</guid>
      <description>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # -*- encoding=utf8 -*- __author__ = &amp;#34;Hank&amp;#34; from airtest.core.api import * from poco.drivers.android.uiautomation import AndroidUiautomationPoco poco = AndroidUiautomationPoco(use_airtest_input=True, screenshot_each_action=False) auto_setup(__file__) import time import random import pandas as pd page_data=[] while True: city=poco(name=&amp;#39;com.lucky.luckyclient:id/tv_select_city&amp;#39;).get_text() result_obj = poco(&amp;#34;com.lucky.luckyclient:id/frameLayout&amp;#34;).offspring(&amp;#34;com.lucky.luckyclient:id/lv_address_list&amp;#34;).child(&amp;#34;android.widget.LinearLayout&amp;#34;) for result in result_obj: brand = result.child(&amp;#34;android.widget.LinearLayout&amp;#34;).child(&amp;#34;android.widget.LinearLayout&amp;#34;).child(name=&amp;#39;com.lucky.luckyclient:id/iv_shop_brand&amp;#39;).get_text() name = result.child(&amp;#34;android.widget.LinearLayout&amp;#34;).child(&amp;#34;android.widget.LinearLayout&amp;#34;).child(name=&amp;#39;com.lucky.luckyclient:id/tv_dept_name&amp;#39;).get_text() address = result.child(&amp;#34;android.widget.LinearLayout&amp;#34;).child(&amp;#34;android.widget.LinearLayout&amp;#34;).child(name=&amp;#39;com.lucky.luckyclient:id/tv_dept_address&amp;#39;).get_text() city=poco(name=&amp;#39;com.lucky.luckyclient:id/tv_select_city&amp;#39;).get_text() data = city, name, address, brand s=str(city)+str(name)+str(address)+str(brand) if &amp;#34;UIObjectProxy&amp;#34; in s: pass else: page_data.append(data) print(data) end = poco(text=&amp;#34;已经全部加载完成&amp;#34;)</description>
    </item>
    
    <item>
      <title>使用Airtest爬取某咖啡app数据v1</title>
      <link>https://hank-leo.github.io/post/app/%E4%BD%BF%E7%94%A8Airtest%E7%88%AC%E5%8F%96%E6%9F%90%E5%92%96%E5%95%A1app%E6%95%B0%E6%8D%AEv2/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/app/%E4%BD%BF%E7%94%A8Airtest%E7%88%AC%E5%8F%96%E6%9F%90%E5%92%96%E5%95%A1app%E6%95%B0%E6%8D%AEv2/</guid>
      <description>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 # -*- encoding=utf8 -*- __author__ = &amp;#34;Hank&amp;#34; import os import time import json import shutil import random import requests import traceback</description>
    </item>
    
    <item>
      <title>使用Github Actions搭建hugo</title>
      <link>https://hank-leo.github.io/post/blog/%E4%BD%BF%E7%94%A8Github-Actions%E6%90%AD%E5%BB%BAhugo/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/blog/%E4%BD%BF%E7%94%A8Github-Actions%E6%90%AD%E5%BB%BAhugo/</guid>
      <description>前言 在本地提交文章到GitHub，会触发Github Actions把刚提交的文章通过hugo发布到github pages，之后可以通过github pages生成的url进行访问。 这里将把hugo源码和pages分别用两个仓库进行管理。 另外，这篇着重将如何使用github actio</description>
    </item>
    
    <item>
      <title>使用异步函数获取文件名</title>
      <link>https://hank-leo.github.io/post/python/%E4%BD%BF%E7%94%A8%E5%BC%82%E6%AD%A5%E5%87%BD%E6%95%B0%E8%8E%B7%E5%8F%96%E6%96%87%E4%BB%B6%E5%90%8D/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/python/%E4%BD%BF%E7%94%A8%E5%BC%82%E6%AD%A5%E5%87%BD%E6%95%B0%E8%8E%B7%E5%8F%96%E6%96%87%E4%BB%B6%E5%90%8D/</guid>
      <description>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 import re import os import time import math import shutil import random import random import asyncio import traceback import pandas as pd from win32com.client import</description>
    </item>
    
    <item>
      <title>利用PyQt5交互导出数据</title>
      <link>https://hank-leo.github.io/post/spider/pyqt/%E5%88%A9%E7%94%A8PyQt5%E4%BA%A4%E4%BA%92%E5%AF%BC%E5%87%BA%E6%95%B0%E6%8D%AE/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/spider/pyqt/%E5%88%A9%E7%94%A8PyQt5%E4%BA%A4%E4%BA%92%E5%AF%BC%E5%87%BA%E6%95%B0%E6%8D%AE/</guid>
      <description>项目说明 MainFunc.py 主文件功能 选取文件(xlsm,csv) 导出match、减少、新增、更新部分的数据 各个部分新开窗口操作 源文件 SubPart.py 减少部分操作文件 状态确认： 关店、装修、尚未开业的数据 做过reconcile的数据 exc1部分的数据 exc2部分的数据 VDR新增数据 来源harv数据 最后导出到csv</description>
    </item>
    
    <item>
      <title>如何建立数据思维框架</title>
      <link>https://hank-leo.github.io/post/analysis/%E5%A6%82%E4%BD%95%E5%BB%BA%E7%AB%8B%E6%95%B0%E6%8D%AE%E6%80%9D%E7%BB%B4%E6%A1%86%E6%9E%B6/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/analysis/%E5%A6%82%E4%BD%95%E5%BB%BA%E7%AB%8B%E6%95%B0%E6%8D%AE%E6%80%9D%E7%BB%B4%E6%A1%86%E6%9E%B6/</guid>
      <description></description>
    </item>
    
    <item>
      <title>如何撰写一份数据分析报告</title>
      <link>https://hank-leo.github.io/post/analysis/%E5%A6%82%E4%BD%95%E6%92%B0%E5%86%99%E4%B8%80%E4%BB%BD%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/analysis/%E5%A6%82%E4%BD%95%E6%92%B0%E5%86%99%E4%B8%80%E4%BB%BD%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/</guid>
      <description></description>
    </item>
    
    <item>
      <title>推断性统计</title>
      <link>https://hank-leo.github.io/post/statistics/%E6%8E%A8%E6%96%AD%E6%80%A7%E7%BB%9F%E8%AE%A1/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/statistics/%E6%8E%A8%E6%96%AD%E6%80%A7%E7%BB%9F%E8%AE%A1/</guid>
      <description>推断统计分析概述 推断的神奇 总体、个体与样本* 推断统计概念 点估计与区间估计 点估计 区间估计 中心极限定理 正态分布的特性 假设检验 概念 小概率事件 P-Value与显著性水平 假设检验的步骤 常用假设检验 $Z$检验 $t$检验</description>
    </item>
    
    <item>
      <title>描述性统计</title>
      <link>https://hank-leo.github.io/post/statistics/%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%BB%9F%E8%AE%A1/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/statistics/%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%BB%9F%E8%AE%A1/</guid>
      <description>数理统计基础 描述性统计分析概述 统计量 频数与频率 集中趋势 均值 中位数 众数 分为数 离散程度 极差 方差 标准差 分布形状 偏度 峰度</description>
    </item>
    
    <item>
      <title>搭建个人数据分析平台</title>
      <link>https://hank-leo.github.io/post/analysis/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%9A%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/analysis/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%9A%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0/</guid>
      <description></description>
    </item>
    
    <item>
      <title>某医院药品销售数据分析实战</title>
      <link>https://hank-leo.github.io/post/analysis/%E6%9F%90%E5%8C%BB%E9%99%A2%E8%8D%AF%E5%93%81%E9%94%80%E5%94%AE%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/analysis/%E6%9F%90%E5%8C%BB%E9%99%A2%E8%8D%AF%E5%93%81%E9%94%80%E5%94%AE%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98/</guid>
      <description></description>
    </item>
    
    <item>
      <title>牛客网SQL刷题记录</title>
      <link>https://hank-leo.github.io/post/sql/%E7%89%9B%E5%AE%A2%E7%BD%91SQL%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/sql/%E7%89%9B%E5%AE%A2%E7%BD%91SQL%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/</guid>
      <description>1.查找最晚入职员工的所有信息 1 2 select * from employees order by hire_date desc limit 0,1; 2.查找入职员工时间排名倒数第三的员工所有信息 1 2 select * from employees order by hire_date desc limit 2,1; 3.查找当前薪水详情以及部门编号dept_no 1 2 3 select salaries.*,dept_manager.dept_no from salaries,dept_manager on salaries.emp_no = dept_manager.emp_no where salaries.to_date=&amp;#39;9999-01-01&amp;#39; and dept_manager.to_date=&amp;#39;9999-01-01&amp;#39;; 4.查找所有已经分配部门的员工的last_name和first_name以及de</description>
    </item>
    
    <item>
      <title>肯德基门店分布可视化分析</title>
      <link>https://hank-leo.github.io/post/analysis/%E8%82%AF%E5%BE%B7%E5%9F%BA%E9%97%A8%E5%BA%97%E5%88%86%E5%B8%83%E5%8F%AF%E8%A7%86%E5%8C%96%E5%88%86%E6%9E%90/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/analysis/%E8%82%AF%E5%BE%B7%E5%9F%BA%E9%97%A8%E5%BA%97%E5%88%86%E5%B8%83%E5%8F%AF%E8%A7%86%E5%8C%96%E5%88%86%E6%9E%90/</guid>
      <description>项目介绍 背景：肯德基快餐公司是全球大型连锁快餐企业，截止至2020年已进驻了中国140多个城市，门店突破7000家。 根据门店地址，使用Pyecharts对肯德基门店分布进行可视化分析： 可视化需要解决的思路: 需要把门店地址转换成经纬度 需要根据省份、城市、详细地址信息得到经纬度 构造G</description>
    </item>
    
    <item>
      <title>Python练习</title>
      <link>https://hank-leo.github.io/post/python/python%E7%BB%83%E4%B9%A0/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/python/python%E7%BB%83%E4%B9%A0/</guid>
      <description>实例001：数字组合 题目： 有四个数字：1、2、3、4，能组成多少个互不相同且无重复数字的三位数？各是多少？ 程序分析： 遍历全部可能，把有重复的剃掉。 total=0 for i in range(1,5): for j in range(1,5): for k in range(1,5): if ((i!=j)and(j!=k)and(k!=i)): print(i,j,k) total+=1 print(total) 简便方法： 用itertools中的permutations即可。 import itertools sum2=0 a=[1,2,3,4] for i in itertools.permutations(a,3): print(i) sum2+=1 print(sum2) 实例002：</description>
    </item>
    
    <item>
      <title>Python面试题</title>
      <link>https://hank-leo.github.io/post/python/Python%E9%9D%A2%E8%AF%95%E9%A2%98/</link>
      <pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/python/Python%E9%9D%A2%E8%AF%95%E9%A2%98/</guid>
      <description>Python语言特性 1 Python的函数参数传递 看两个例子: a = 1 def fun(a): a = 2 fun(a) print a # 1 a = [] def fun(a): a.append(1) fun(a) print a # [1] 所有的变量都可以理解是内存中一个对象的“引用”，或者，也可以看似c中void*的感觉。 通过id来看引用a的内存地址可以比较理解： a = 1 def fun(a): print &amp;quot;func_in&amp;quot;,id(a) # func_in 41322472 a = 2 print &amp;quot;re-point&amp;quot;,id(a), id(2) # re-point 41322448 41322448 print &amp;quot;func_out&amp;quot;,id(a),</description>
    </item>
    
    <item>
      <title>爬虫面试题</title>
      <link>https://hank-leo.github.io/post/spider/%E7%88%AC%E8%99%AB%E9%9D%A2%E8%AF%95%E9%A2%98/</link>
      <pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/spider/%E7%88%AC%E8%99%AB%E9%9D%A2%E8%AF%95%E9%A2%98/</guid>
      <description>基础 http协议, tcp协议 top 命令 Linux/Mac 下虚拟内存 (Swap) 用过的linux命令，具体细节比如crontab命令报错可能有哪些原因 线程、进程、协程 Async 相关、事件驱动相关 阻塞、非阻塞 Python GIL 布隆过滤器原理：如何实现、一般要几次哈希函数 MySQL的索引底层 实战 抓取天猫超市上某些商品配送省份信息 (主要需</description>
    </item>
    
    <item>
      <title>使用hexo搭建个人博客</title>
      <link>https://hank-leo.github.io/post/blog/%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</link>
      <pubDate>Fri, 06 Dec 2019 21:23:32 +0800</pubDate>
      
      <guid>https://hank-leo.github.io/post/blog/%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</guid>
      <description>新建项目 首先在 GitHub 新建一个仓库Repository, 名称为username.githu.io, 注意这个名比较特殊, 必须要是github.io为后缀结尾的。要与用户名一致，比如ihankleo的 GitHub 用户名就叫ihankleo.github.io, 新建完成之后就可以进行后续操作了。 在此之</description>
    </item>
    
    <item>
      <title>Scrapy框架:Scrapy&#43;MongoDB实战:抓取并保存IT之家博客新闻</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Scrapy&#43;MongoDB%E5%AE%9E%E6%88%98%E6%8A%93%E5%8F%96%E5%B9%B6%E4%BF%9D%E5%AD%98IT%E4%B9%8B%E5%AE%B6%E5%8D%9A%E5%AE%A2%E6%96%B0%E9%97%BB/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Scrapy&#43;MongoDB%E5%AE%9E%E6%88%98%E6%8A%93%E5%8F%96%E5%B9%B6%E4%BF%9D%E5%AD%98IT%E4%B9%8B%E5%AE%B6%E5%8D%9A%E5%AE%A2%E6%96%B0%E9%97%BB/</guid>
      <description>创建项目 scrapy startproject ithome cd ithome scrapy genspider -t crawl news ithome.com 编写items.py文件 # -*- coding: utf-8 -*- # Define here the models for your scraped items # # See documentation in: # https://doc.scrapy.org/en/latest/topics/items.html import scrapy class IthomeItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() #文章标题 title = scrapy.Field() #文章url url = scrapy.Field() #来源 source = scrapy.Field() #来源url source_url = scrapy.Field() #发布日期 release_大特＝ scrapy.Field() #作者 author = scrapy.Field() #关键词 key_words = scrapy.Field() 编写爬虫文件 news.py # -*- coding: utf-8 -*- import</description>
    </item>
    
    <item>
      <title>Scrapy框架:通用爬虫</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB/</guid>
      <description>通用爬虫之CrawlSpider 步骤01: 创建爬虫项目 scrapy startproject quotes 步骤02: 创建爬虫模版 scrapy genspider -t quotes quotes.toscrape.com 步骤03: 配置爬虫文件quotes.py import scrapy from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors import LinkExtractor class Quotes(CrawlSpider): # 爬虫名称 name = &amp;quot;get_quotes&amp;quot; allow_domain = [&#39;quotes.toscrape.com&#39;] start_urls = [&#39;http://quotes.toscrape.com/&#39;] # 设定规则 rules = ( # 对于quotes内容页URL，调用parse_quotes处理， # 并以此规则</description>
    </item>
    
    <item>
      <title>使用Airtest进行App爬虫</title>
      <link>https://hank-leo.github.io/post/app/%E4%BD%BF%E7%94%A8Airtest%E8%BF%9B%E8%A1%8CApp%E7%88%AC%E8%99%AB/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/app/%E4%BD%BF%E7%94%A8Airtest%E8%BF%9B%E8%A1%8CApp%E7%88%AC%E8%99%AB/</guid>
      <description>前言 Airtest是网易开发的手机UI戒面自动化测试工具， 安装 从Airtest官网 : https://airtest.netease.com 下载Airtest，然后像安装普通软件一样安装即可。 手机连接 启动Airtest以后，把Android手机连接到电脑上，点击下图方框中的refresh ADB 然后点击connect按钮 开始爬虫 以某品牌咖</description>
    </item>
    
    <item>
      <title>存储数据之excel</title>
      <link>https://hank-leo.github.io/post/spider/%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E4%B9%8Bexcel/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/spider/%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E4%B9%8Bexcel/</guid>
      <description>第一种方式是使用pandas库 def save(data): data = pd.DataFrame(data) data.to_excel(index=False) 第二种方式是使用openpyxl库 import openpyxl def save(title, head, data): outwb = openpyxl.Workbook() outws = outwb.active outws.title=title for h in range(len(head)): outws.cell(1,h+1).value=head[h] for row in data: outws.append(row) outwb.save(&amp;quot;path&amp;quot;) 第三种方式是使用xlwt库 def save(title, head, data): workbook = xlwt.Workbook(encoding=&#39;utf8&#39;) sheet = workbook.add_sheet(title, cell_overwrite_ok=True) for h in range(len(head)): sheet.write(0, h, head[h]) i = 1 for list in data: j = 0 for data in list: sheet.write(i, j, data) j += 1 i += 1 workbook.save(path)</description>
    </item>
    
    <item>
      <title>解析与提取数据之re</title>
      <link>https://hank-leo.github.io/post/spider/%E8%A7%A3%E6%9E%90%E4%B8%8E%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B9%8Bre/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/spider/%E8%A7%A3%E6%9E%90%E4%B8%8E%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B9%8Bre/</guid>
      <description>对比xpath, re对文本结构的数据处理更加灵活 #导入包 import re def get_data(response): #提取内容 title = re.findall(&amp;quot;^&amp;lt;span&amp;gt;(.*?)&amp;lt;/span&amp;gt;&amp;quot;,response, re.S)</description>
    </item>
    
    <item>
      <title>解析与提取数据之xpath</title>
      <link>https://hank-leo.github.io/post/spider/%E8%A7%A3%E6%9E%90%E4%B8%8E%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B9%8Bxpath/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/spider/%E8%A7%A3%E6%9E%90%E4%B8%8E%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B9%8Bxpath/</guid>
      <description>日常工作主要使用xpath进行数据的解析和提取，归于xpath有着强大的功能，适应大多数的网页结构。 #导入包 from lxml import etree def get_data(response): html=etree.HTML(response) #提取属性值 url=html.xpath(&amp;quot;//div[@href]&amp;quot;) #提取文本 title=html.xpath(&amp;quot;//div[@class=&#39;title&#39;]/text()&amp;quot;)</description>
    </item>
    
    <item>
      <title>MongoDB的实际应用</title>
      <link>https://hank-leo.github.io/post/sql/MongoDB%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/sql/MongoDB%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</guid>
      <description>MongoDB基本操作 1.数据库操作 使用use DATABASE_NAME创建数据库 use maitian 如果数据库不存在，就创建数据库，否则切换到指定的数据库 使用show dbs查看所有数据库 show dbs 使用db.dropDatabase()删除数据库 use maitian db.dropDatabase() 2.集合操作 在maitian数据库中创建名为zuf</description>
    </item>
    
    <item>
      <title>Xpath的实际应用</title>
      <link>https://hank-leo.github.io/post/python/xpath%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/python/xpath%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</guid>
      <description>xpath中使用contains xpath(span[contanins(@class, &#39;xxx&#39;)]) Xpath如何选择不包含某一个属性的节点? 这里可以用到 not 例如排除一个属性的节点可以使用 //tbody/tr[not(@class)] 排除一个或者两个属性可以使用 //tbody/tr[not(@class or @id)] xpath按序选择 有时候我们在选择的时候可能某些属性同时匹配了多个节点，但是我们只想要其中的某个节点，如第二个节点，或者最</description>
    </item>
    
    <item>
      <title>使用hugo搭建个人博客</title>
      <link>https://hank-leo.github.io/post/blog/%E4%BD%BF%E7%94%A8hugo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/blog/%E4%BD%BF%E7%94%A8hugo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</guid>
      <description>安装 在这个页面下载安装https://github.com/gohugoio/hugo/releases 下载完成后解压，将解压出来的可执行文件，放到自定义目录下，并将选择的路径放入环境变量path中 初始化 下面开始存放我们的博客，在选好的路径下执行 hugo new site ihankleo.github.io 命令执行完会创建一个名为m</description>
    </item>
    
    <item>
      <title>列表的实际应用</title>
      <link>https://hank-leo.github.io/post/python/%E5%88%97%E8%A1%A8%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/python/%E5%88%97%E8%A1%A8%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</guid>
      <description>列表合并保留最大长度 import itertools w, x, y, z = [], [1], [2, 3], [4, 5, 6] longest_wxyz = itertools.zip_longest(w, x, y, z) print(list(longest_wxyz)) 结果: [(None, 1, 2, 4), (None, None, 3, 5), (None, None, None, 6)] 列表元素替换 lst = [&#39;1&#39;,&#39;2&#39;,&#39;3&#39;] rep = [&#39;4&#39; if x == &#39;2&#39; else x for x in lst] print(rep) 结果: [&amp;lsquo;1&amp;rsquo;, &amp;lsquo;4&amp;rsquo;, &amp;lsquo;3&amp;rsquo;] 列表进行去重操作 一般的去重操作后是出现乱序的情况 t=[&#39;8&#39;,&#39;7&#39;,&#39;2&#39;,&#39;中国&#39;,&#39;China&#39;,&#39;中国&#39;,&#39;1&#39;,&#39;4&#39;]</description>
    </item>
    
    <item>
      <title>字典的实际应用</title>
      <link>https://hank-leo.github.io/post/python/%E5%AD%97%E5%85%B8%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/python/%E5%AD%97%E5%85%B8%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</guid>
      <description>1.字典排序 利用key排序 d = {&#39;d1&#39;:2, &#39;d2&#39;:4, &#39;d4&#39;:1,&#39;d3&#39;:3,} for k in sorted(d): print(k,d[k]) d1 2 d2 4 d3 3 d4 1 利用value排序：getitem d = {&#39;d1&#39;:2, &#39;d2&#39;:4, &#39;d4&#39;:1,&#39;d3&#39;:3,} for k in sorted(d,key=d.__getitem__): print(k,d[k]) d4 1 d1 2 d3 3 d2 4 反序: reverse=True d = {&#39;d1&#39;:2, &#39;d2&#39;:4, &#39;d4&#39;:1,&#39;d3&#39;:3,} for k in sorted(d,key=d.__getitem__,reverse=True): print(k,d[k]) d2 4 d3 3 d1 2 d4 1 对dict_items进行排序 d = {&#39;d1&#39;:2, &#39;d2&#39;:4, &#39;d4&#39;:1,&#39;d3&#39;:3,} res = sorted(d.items(),key=lambda d:d[1],reverse=True) print(res) [(&amp;lsquo;d2&amp;rsquo;, 4), (&amp;lsquo;d3&amp;rsquo;, 3), (&amp;lsquo;d1&amp;rsquo;, 2), (&amp;lsquo;d4&amp;rsquo;, 1)] 2.两个字典（dict）合并 dict1 = { &amp;quot;name&amp;quot;:&amp;quot;owen&amp;quot;, &amp;quot;age&amp;quot;:</description>
    </item>
    
    <item>
      <title>字符串的实际应用</title>
      <link>https://hank-leo.github.io/post/python/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/python/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</guid>
      <description>1.判断字符串是否为小数 try: lat = float(location.split(&#39;,&#39;)[1]) lon = float(location.split(&#39;,&#39;)[0]) except ValueError: print(&#39;no number&#39;) 2.用split对字符串进行分割 str=&#39;storeId=ff8080816277aa0a0162845d48e3012b&amp;amp;appid=wxe37b2e703155ed41&amp;amp;transId=wxe37b2e703155ed412019-05-09%2010%3A28%3A15&amp;amp;sign=dc8fac903b03556247659e1b548bccce&amp;amp;timestamp=2019-05-09%2010%3A28%3A15&amp;amp;memberId=ff8080816a889e71016a9a68a6c55a37&amp;amp;cliqueId=-1&amp;amp;cliqueMemberId=-1&amp;amp;useClique=0&amp;amp;enterpriseId=ff808081624e60f601625c50a30900ce&amp;amp;unionid=oLWn80pR0DtSJXfnO_1O4ZOzfvAE&amp;amp;openid=oZe8D5gmPcPANw4kNNcG8mlAW1mI&amp;amp;launchOptions=%7B%22path%22%3A%22%2Fpages%2Fmall%2Fmall-index%2Fmall-index%22%2C%22query%22%3A%7B%7D%2C%22scene%22%3A1102%2C%22referrerInfo%22%3A%7B%22appId%22%3A%22wx97e5123eb6041454%22%7D%7D&#39; str2=str.split(&#39;&amp;amp;&#39;) for i in str2: print(&#39;&amp;quot;&#39;+i.split(&#39;=&#39;)[0]+&#39;&amp;quot;:&amp;quot;&#39;+i.split(&#39;=&#39;)[1]+&#39;&amp;quot;,&#39;) 结果如下： &amp;ldquo;storeId&amp;rdquo;:&amp;ldquo;ff8080816277aa0a0162845d48e3012b&amp;rdquo;, &amp;ldquo;appid&amp;rdquo;:&amp;ldquo;wxe37b2e703155ed41&amp;rdquo;, &amp;ldquo;transId&amp;rdquo;:&amp;ldquo;wxe37b2e703155ed412019-05-09%2010%3A28%3A15&amp;rdquo;, &amp;ldquo;sign&amp;rdquo;:&amp;ldquo;dc8fac903b03556247659e1b548bccce&amp;rdquo;, &amp;ldquo;timestamp&amp;rdquo;:&amp;ldquo;2019-05-09%2010%3A28%3A15&amp;rdquo;, &amp;ldquo;memberId&amp;rdquo;:&amp;ldquo;ff8080816a889e71016a9a68a6c55a37&amp;rdquo;, &amp;ldquo;cliqueId&amp;rdquo;:&amp;quot;-1&amp;rdquo;, &amp;ldquo;cliqueMemberId&amp;rdquo;:&amp;quot;-1&amp;rdquo;, &amp;ldquo;useClique&amp;rdquo;:&amp;ldquo;0&amp;rdquo;, &amp;ldquo;enterpriseId&amp;rdquo;:&amp;ldquo;ff808081624e60f601625c50a30900ce&amp;rdquo;, &amp;ldquo;unionid&amp;rdquo;:&amp;ldquo;oLWn80pR0DtSJXfnO_1O4ZOzfvAE&amp;rdquo;, &amp;ldquo;openid&amp;rdquo;:&amp;ldquo;oZe8D5gmPcPANw4kNNcG8mlAW1mI&amp;rdquo;, &amp;ldquo;launchOptions&amp;rdquo;:&amp;quot;%7B%22path%22%3A%22%2Fpages%2Fmall%2Fmall-index%2Fmall-index%22%2C%22query%22%3A%7B%7D%2C%22scene%22%3A1102%2C%22referrerInfo%22%3A%7B%22appId%22%3A%22wx97e5123eb6041454%22%7D%7D&amp;rdquo;, 3.python检测字符串乱码 import chardet f=open(&#39;test.txt&#39;,&#39;rb&#39;) f_read=f.read() f_charInfo=chardet.detect(f_read) print(f_charInfo) # f_charInfo的输出是这样的的一个字典{&#39;confidence&#39;: 0.99, &#39;encoding&#39;: &#39;utf-8&#39;} 4.将逗号分隔的字符串转换为P</description>
    </item>
    
    <item>
      <title>正则的实际应用</title>
      <link>https://hank-leo.github.io/post/python/%E6%AD%A3%E5%88%99%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/python/%E6%AD%A3%E5%88%99%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</guid>
      <description>grep hank@hank:~$ cat grep.txt ooxx121212121ooxx ooxx 12121212 oox 12121212 1212 ooxx 1212 oo3xx oo4xx ooWxx oomxx $ooxx oo1234xx ooxyzxx hank@hank:~$ grep &amp;quot;ooxx&amp;quot; grep.txt ooxx121212121ooxx ooxx 12121212 1212 ooxx 1212 $ooxx hank@hank:~$ grep &amp;quot;[34]&amp;quot; grep.txt oo3xx oo4xx oo1234xx hank@hank:~$ grep &amp;quot;[0-9]\{4\}&amp;quot; grep.txt ooxx121212121ooxx ooxx 12121212 oox 12121212 1212 ooxx 1212 oo1234xx hank@hank:~$ grep -E &amp;quot;[0-9]{4}&amp;quot; grep.txt ooxx121212121ooxx ooxx 12121212 oox 12121212 1212 ooxx 1212 oo1234xx hank@hank:~$ grep &amp;quot;\&amp;lt;ooxx\&amp;gt;&amp;quot; grep.txt ooxx 12121212 1212 ooxx 1212 $ooxx hank@hank:~$ grep &amp;quot;[^0-9][0-9]\{4\}[^0-9]&amp;quot; grep.txt oo1234xx hank@hank:~$ grep &amp;quot;\(^[0-9]\|[^0-9][0-9]\)[0-9]\{2\}\([0-9]$\|[0-9][^0-9]\)&amp;quot; grep.txt 1212 ooxx 1212 oo1234xx hank@hank:~$ grep -E &amp;quot;(^[0-9]|[^0-9][0-9])[0-9]{2}([0-9]$|[0-9][^0-9])&amp;quot; grep.txt 1212 ooxx 1212 oo1234xx</description>
    </item>
    
    <item>
      <title>MySQL的实际应用</title>
      <link>https://hank-leo.github.io/post/sql/MySQL%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/sql/MySQL%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</guid>
      <description>创建触发器语法 create trigger tgName after/before insert/delete/update on tableName for each row sql; -- 触发语句 删除触发器 drop trigger tgName; 索引 提高查询速度,但是降低了增删改的速度, 所以使用索引时,要综合考虑. 索引不是越多越好,一般我们在常出现于条件表达式中的列加索引. 值越分散的列，索引的效果越好 索引类型 |索引|解释 :-:|:-: primary key|主键索引 index|普通索引</description>
    </item>
    
    <item>
      <title>Pandas异常值处理</title>
      <link>https://hank-leo.github.io/post/analysis/Pandas%E5%BC%82%E5%B8%B8%E5%80%BC%E5%A4%84%E7%90%86/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/analysis/Pandas%E5%BC%82%E5%B8%B8%E5%80%BC%E5%A4%84%E7%90%86/</guid>
      <description>导包 import pandas as pd 生成异常数据 df=pd.DataFrame({&#39;col1&#39;:[1,120,3,5,2,12,13],&#39;col2&#39;:[12,17,31,53,22,32,43]}) #打印 print(df) col1 col2 0 1 12 1 120 17 2 3 31 3 5 53 4 2 22 5 12 32 6 13 43 df_zscore=df.copy() #复制一个用来存储Z-score得分的数据框 cols=df.columns for col in cols: df_col=df[col] z_score=(df_col - df_col.mean()) / df_col.std() #计算每列的Z-score得分 df_zscore[col] = z_score.abs() &amp;gt; 2.2 #判断Z-score得分是否大于2.2,如果是则为True,否则为False #打印，为Tru</description>
    </item>
    
    <item>
      <title>Pandas缺失值处理</title>
      <link>https://hank-leo.github.io/post/analysis/Pandas%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/analysis/Pandas%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86/</guid>
      <description>导入库 import pandas as pd import numpy as np from sklearn.preprocessing import Imputer 生成缺失数据 df=pd.DataFrame(np.random.randn(6,4),columns=[&#39;col1&#39;,&#39;col2&#39;,&#39;col3&#39;,&#39;col4&#39;]) df.iloc[1:2,1] = np.nan #增加缺失值 df.iloc[4,3] = np.nan #增加缺失值 #打印输出 print(df) col1 col2 col3 col4 0 -0.977511 -0.566332 -0.529934 1.489695 1 -0.491128 NaN -0.811174 -1.102717 2 0.385777 -0.638822 0.325953 -0.240780 3 0.938351 -0.746889 0.375200 -0.715265 4 1.103418 0.238959 -0.459114 NaN 5 1.002177 0.448844 -0.584634 -1.038151 查看缺失值位置 nan_all=df.isnull() #打印 print(nan_all) col1 col2 col3 col4 0 False False False False 1 False True False False 2 False False False False 3 False False False False 4 False False False True 5 False False False False #获取含有NA的列 nan_col1=df.isnull().any() #打印 print(nan_col1)</description>
    </item>
    
    <item>
      <title>Pandas重复值处理</title>
      <link>https://hank-leo.github.io/post/analysis/Pandas%E9%87%8D%E5%A4%8D%E5%80%BC%E5%A4%84%E7%90%86/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/analysis/Pandas%E9%87%8D%E5%A4%8D%E5%80%BC%E5%A4%84%E7%90%86/</guid>
      <description>导包 import pandas as pd 生成数据 data1,data2,data3,data4=[&#39;a&#39;,3],[&#39;b&#39;,2],[&#39;a&#39;,3],[&#39;c&#39;,2] df=pd.DataFrame([data1,data2,data3,data4],columns=[&#39;col1&#39;,&#39;col2&#39;]) print(df) col1 col2 0 a 3 1 b 2 2 a 3 3 c 2 判断数据 isDuplicated=df.duplicated() #判断重复数据记录 print(isDuplicated) 0 False 1 False 2 True 3 False dtype: bool 删除重复的数据 print(df.drop_duplicates()) #删除所有列值相同的记录，index为2的记录行被删除 col1 col2 0 a 3 1 b 2 3 c 2 #删除col1列值相同的记录，index为2的记录行被删除 print(df.drop_duplicates([&#39;col1&#39;])) col1 col2 0 a 3 1 b 2 3 c 2 #</description>
    </item>
    
    <item>
      <title>Scrapy框架 基本命令</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/</guid>
      <description>创建爬虫项目 scrapy startproject [项目名称] 创建爬虫文件 scrapy genspider +文件名+网址 运行(crawl) scrapy crawl 爬虫名称 # -o output 输出数据到文件 scrapy crawl [爬虫名称] -o zufang.json scrapy crawl [爬虫名称] -o zufang.csv check检查错误 scrapy check list返回项目所有spider scrapy list view 存储、打开网页 scrapy view http://www.baidu.com scrapy shell, 进入终端 scrapy shell https://www.baidu.com scrapy runspider scrapy runspider zufang_spider.py</description>
    </item>
    
    <item>
      <title>Scrapy框架 登录网站</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%99%BB%E5%BD%95%E7%BD%91%E7%AB%99/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%99%BB%E5%BD%95%E7%BD%91%E7%AB%99/</guid>
      <description>使用cookies登录网站 import scrapy class LoginSpider(scrapy.Spider): name = &#39;login&#39; allowed_domains = [&#39;xxx.com&#39;] start_urls = [&#39;https://www.xxx.com/xx/&#39;] cookies = &amp;quot;&amp;quot; def start_requests(self): for url in self.start_urls: yield scrapy.Request(url, cookies=self.cookies, callback=self.parse) def parse(self, response): with open(&amp;quot;01login.html&amp;quot;, &amp;quot;wb&amp;quot;) as f: f.write(response.body) 发送post请求登录, 要手动解析网页获取登录参数 import scrapy class LoginSpider(scrapy.Spider): name=&#39;login_code&#39; allowed_domains = [&#39;xxx.com&#39;] #1. 登录页面 start_urls = [&#39;https://www.xxx.com/login/&#39;] def parse(self, response): #2. 代码登录 login_url=&#39;https://www.xxx.com/login&#39; formdata={ &amp;quot;username&amp;quot;:&amp;quot;xxx&amp;quot;, &amp;quot;pwd&amp;quot;:&amp;quot;xxx&amp;quot;, &amp;quot;formhash&amp;quot;:response.xpath(&amp;quot;//input[@id=&#39;formhash&#39;]/@value&amp;quot;).extract_first(), &amp;quot;backurl&amp;quot;:response.xpath(&amp;quot;//input[@id=&#39;backurl&#39;]/@value&amp;quot;).extract_first() } #3. 发送登录请求post yield scrapy.FormRequest(login_url, formdata=formdata, callback=self.parse_login) def parse_login(self, response): #4.访问目标页面 member_url=&amp;quot;https://www.xxx.com/member&amp;quot; yield scrapy.Request(member_url, callback=self.parse_member) def parse_member(self, response): with open(&amp;quot;02login.html&amp;quot;,&#39;wb&#39;) as</description>
    </item>
    
    <item>
      <title>Scrapy框架:Request回调函数</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Request%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Request%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0/</guid>
      <description>Request回调函数 def parse_page1(self, response): return scrapy.Request(&amp;quot;http://www.example.com/some_page.html&amp;quot;, callback=self.parse_page2) def parse_page2(self, response): # this would log http://www.example.com/some_page.html self.logger.info(&amp;quot;Visited %s&amp;quot;, response.url) 传递参数 def parse_page1(self, response): item = MyItem() item[&#39;name&#39;] = response.css(&#39;.name::text&#39;).extract_first() request = scrapy.Request(&amp;quot;http://www.example.com/some_page.html&amp;quot;, callback=self.parse_page2) request.meta[&#39;item&#39;] = item yield request def parse_page2(self, response): item = response.meta[&#39;item&#39;] item[&#39;age&#39;] = response.css(&#39;.age::text&#39;).extract_first() yield item</description>
    </item>
    
    <item>
      <title>Scrapy框架:Settings.py</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-settings-py/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-settings-py/</guid>
      <description>#Scrapy项目名字 BOT_NAME = &#39;segmentfault&#39; #Scrapy搜索spider的模块列表 SPIDER_MODULES = [&#39;segmentfault.spiders&#39;] #使用爬虫创建命令genspider创建爬虫时生成的模块 NEWSPIDER_MODULE = &#39;segmentfault.spiders&#39; #默认的USER_AGENT, 使用BOT_NAME配置生成，建议覆盖 #USER_AGNET = &#39;segmentfault (+http://www.yourdomain.com)&#39; #如果启用，Scrapy则会遵守网站Rebots.txt协议，建议设</description>
    </item>
    
    <item>
      <title>Scrapy框架:入门案例</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/</guid>
      <description>创建项目: scrappy start project maitian 明确要抓取的字段items.py import scrapy class MaitianItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() title = scrapy.Field() price = scrapy.Field() area = scrapy.Field() district = scrapy.Field() 在spider目录下创建爬虫文件: zufang_spider.py 2.1 创建一个类，并继承scrapy的一个子类: scrapy.Spider 2.2 自定义爬取名, name=&amp;quot;&amp;quot; 后面运行框架需要用到； 2.3 定义爬取目标网址 2.4 定义scrapy的方法 下面是简</description>
    </item>
    
    <item>
      <title>Scrapy框架:异常处理</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/</guid>
      <description>import scrapy from scrapy.spidermiddlewares.httperror import HttpError from twisted.internet.error import DNSLookupError from twisted.internet.error import TimeoutError, TCPTimedOutError class ErrbackSpider(scrapy.Spider): name = &amp;quot;errback_example&amp;quot; start_urls = [ &amp;quot;http://www.httpbin.org/&amp;quot;, # 正常HTTP 200返回 &amp;quot;http://www.httpbin.org/status/404&amp;quot;, # 404 Not found error &amp;quot;http://www.httpbin.org/status/500&amp;quot;, # 500服务器错误 &amp;quot;http://www.httpbin.org:12345/&amp;quot;, # 超时无响应错误 &amp;quot;http://www.httphttpbinbin.org/&amp;quot;, # DNS 错误 ] def start_requests(self): for u in self.start_urls: yield scrapy.Request(u, callback=self.parse_httpbin, errback=self.errback_httpbin, dont_filter=True) def parse_httpbin(self, response): self.logger.info(&#39;Got successful response from {}&#39;.format(response.url)) # 其他处理. def errback_httpbin(self, failure): # 日志记录所有的异常信息 self.logger.error(repr(failure)) # 假设我们需要对指定的异常类型做处理， # 我们需要判断异常的类型 if</description>
    </item>
    
    <item>
      <title>Scrapy框架:抓取猫眼电影TOP100</title>
      <link>https://hank-leo.github.io/post/scrapy/scrapy%E6%A1%86%E6%9E%B6-%E6%8A%93%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1TOP100/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/scrapy%E6%A1%86%E6%9E%B6-%E6%8A%93%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1TOP100/</guid>
      <description>需求： 抓取的数据为电影名称、主演、上映日期、评分。将抓取的数据保存到maoyantop100.json文件，并将文件作为附件通过邮件发送给接收人。 创建项目 scrapy startproject maoyan scrapy genspider -t crawl top100 maoyan.com 编写items.py # -*- coding: utf-8 -*- import scrapy class MaoyanItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 电影名称 name = scrapy.Field() # 主演 actors = scrapy.Field() # 上映时间 releasetime = scrapy.Field()</description>
    </item>
    
    <item>
      <title>Scrapy框架:爬取链家二手房信息</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%88%AC%E5%8F%96%E9%93%BE%E5%AE%B6%E4%BA%8C%E6%89%8B%E6%88%BF%E4%BF%A1%E6%81%AF/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%88%AC%E5%8F%96%E9%93%BE%E5%AE%B6%E4%BA%8C%E6%89%8B%E6%88%BF%E4%BF%A1%E6%81%AF/</guid>
      <description>创建爬虫项目 scrapy startproject lianjiahouse 创建爬虫文件 scrapy genspider -t craw house lianjia.com 编写items.py文件 # -*- coding: utf-8 -*- # Define here the models for your scraped items # # See documentation in: # https://doc.scrapy.org/en/latest/topics/items.html import scrapy class LianjiahouseItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 发布信息名称 house_name = scrapy.Field() # 小区名称 community_name = scrapy.Field() # 所在区域 # location = scrapy.Field() # 链家编号 house_record = scrapy.Field() # 总售价 total_amount = scrapy.Field() # 单价 unit_price = scrapy.Field() # 房屋基本信息 # 建筑面积 area_total = scrapy.Field() # 套内面积 area_use</description>
    </item>
    
    <item>
      <title>Scrapy框架:统计数据收集</title>
      <link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/</guid>
      <description>通过stats属性来使用数据收集器 class ExtensionThatAccessStats(object): def __init__(self, stats): self.stats = stats @classmethod def from_crawler(cls, crawler): return cls(crawler.stats) 设置数据 stats.set_value(&#39;hostname&#39;, socket.gethostname()) 增加数据值 stats.inc_value(&#39;pages_crawled&#39;) 当新的值比原来的值大时设置数据 stats.max_value(&#39;max_items_scraoed&#39;, value) 当新的值比原来的值小时设置数据 stats.min_value(&#39;min_free_memory_percent&#39;, value) 获取数据 stats.get_value(&#39;pages_crawled&#39;) 获取所有数据 stats.get_stats() 示例：统计名人名言网站(http://quotes.toscrape.com/)标签为love的名言数</description>
    </item>
    
  </channel>
</rss>