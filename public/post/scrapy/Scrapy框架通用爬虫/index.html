<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Scrapy框架:通用爬虫 | Hank's Blog</title><meta property="og:title" content="Scrapy框架:通用爬虫 - Hank's Blog"><meta property="og:type" content="article"><meta property="article:published_time" content="2019-12-06T00:00:00+08:00"><meta property="article:modified_time" content="2019-12-06T00:00:00+08:00"><meta name=Keywords content="Hank,博客,python"><meta name=description content="Scrapy框架:通用爬虫"><meta name=author content="Hank"><meta property="og:url" content="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB/"><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=stylesheet href=/css/normalize.css><link rel=stylesheet href=/css/style.css><script type=text/javascript src=//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js></script><link rel=stylesheet href=/css/douban.css><link rel=stylesheet href=/css/other.css></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><a id=logo href=https://hank-leo.github.io>Hank's Blog</a><p class=description>专注于Python、JavaScript、SQL、爬虫、数据分析与挖掘</p></div><div><nav id=nav-menu class=clearfix><a class=current href=https://hank-leo.github.io>首页</a>
<a href=https://hank-leo.github.io/archives/ title=归档>归档</a>
<a href=https://hank-leo.github.io/tags/ title=标签>标签</a>
<a href=https://hank-leo.github.io/categories/ title=分类>分类</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><style type=text/css>.post-toc{position:fixed;width:200px;margin-left:-210px;padding:5px 10px;font-family:Athelas,STHeiti,Microsoft Yahei,serif;font-size:12px;border:1px solid rgba(0,0,0,.07);border-radius:5px;background-color:rgba(255,255,255,.98);background-clip:padding-box;-webkit-box-shadow:1px 1px 2px rgba(0,0,0,.125);box-shadow:1px 1px 2px rgba(0,0,0,.125);word-wrap:break-word;white-space:nowrap;-webkit-box-sizing:border-box;box-sizing:border-box;z-index:999;cursor:pointer;max-height:70%;overflow-y:auto;overflow-x:hidden}.post-toc .post-toc-title{width:100%;margin:0 auto;font-size:20px;font-weight:400;text-transform:uppercase;text-align:center}.post-toc .post-toc-content{font-size:15px}.post-toc .post-toc-content>nav>ul{margin:10px 0}.post-toc .post-toc-content ul{padding-left:20px;list-style:square;margin:.5em;line-height:1.8em}.post-toc .post-toc-content ul ul{padding-left:15px;display:none}@media print,screen and (max-width:1057px){.post-toc{display:none}}</style><div class=post-toc style=position:absolute;top:188px><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><ul><li></li></ul></li></ul></nav></div></div><script type=text/javascript>$(document).ready(function(){var postToc=$(".post-toc");if(postToc.length){var leftPos=$("#main").offset().left;if(leftPos<220){postToc.css({"width":leftPos-10,"margin-left":(0-leftPos)})}
var t=postToc.offset().top-20,a={start:{position:"absolute",top:t},process:{position:"fixed",top:20},};$(window).scroll(function(){var e=$(window).scrollTop();e<t?postToc.css(a.start):postToc.css(a.process)})}})</script><article class=post><header><h1 class=post-title>Scrapy框架:通用爬虫</h1></header><date class="post-meta meta-date">2019年12月6日</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=https://hank-leo.github.io/categories/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0>Scrapy框架学习</a></span></div><div class=post-meta><span id=busuanzi_container_page_pv>|<span id=busuanzi_value_page_pv></span><span>
阅读</span></span></div><div class=post-content><h4 id=通用爬虫之crawlspider>通用爬虫之CrawlSpider</h4><p>步骤01: 创建爬虫项目</p><pre><code>scrapy startproject quotes
</code></pre><p>步骤02: 创建爬虫模版</p><pre><code>scrapy genspider -t quotes quotes.toscrape.com
</code></pre><p>步骤03: 配置爬虫文件quotes.py</p><pre><code>import scrapy
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor


class Quotes(CrawlSpider):
	# 爬虫名称
    name = &quot;get_quotes&quot;
    allow_domain = ['quotes.toscrape.com']
    start_urls = ['http://quotes.toscrape.com/']

# 设定规则
    rules = (
        # 对于quotes内容页URL，调用parse_quotes处理，
      		# 并以此规则跟进获取的链接
        Rule(LinkExtractor(allow=r'/page/\d+'), callback='parse_quotes', follow=True),
      		# 对于author内容页URL，调用parse_author处理，提取数据
        Rule(LinkExtractor(allow=r'/author/\w+'), callback='parse_author')
    )

# 提取内容页数据方法
    def parse_quotes(self, response):
        for quote in response.css(&quot;.quote&quot;):
            yield {'content': quote.css('.text::text').extract_first(),
                   'author': quote.css('.author::text').extract_first(),
                   'tags': quote.css('.tag::text').extract()
                   }
	# 获取作者数据方法

    def parse_author(self, response):
        name = response.css('.author-title::text').extract_first()
        author_born_date = response.css('.author-born-date::text').extract_first()
        author_bron_location = response.css('.author-born-location::text').extract_first()
        author_description = response.css('.author-description::text').extract_first()

        return ({'name': name,
                 'author_bron_date': author_born_date,
                 'author_bron_location': author_bron_location,
                 'author_description': author_description
                 })
</code></pre><p>步骤04: 运行爬虫</p><pre><code>scrapy crawl quotes
</code></pre><h4 id=通用爬虫之csvfeedspider>通用爬虫之CSVFeedSpider</h4><p>步骤01: 创建项目</p><pre><code>scrapy startproject csvfeedspider
</code></pre><p>步骤02: 使用csvfeed模版</p><pre><code>scrapy genspider -t csvfeed csvdata gzdata.gov.cn
</code></pre><p>步骤03: 编写items.py</p><pre><code># -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# https://doc.scrapy.org/en/latest/topics/items.html

import scrapy


class CsvspiderItem(scrapy.Item):
    # define the fields for your item here like:
    # 姓名
    name = scrapy.Field()
    # 研究领域
    SearchField = scrapy.Field()
    # 服务分类
    Service = scrapy.Field()
    # 专业特长
    Specialty = scrapy.Field()
</code></pre><p>步骤04: 编写爬虫文件csvdata.py</p><pre><code># -*- coding: utf-8 -*-
from scrapy.spiders import CSVFeedSpider
from csvfeedspider.items import CsvspiderItem


class CsvparseSpider(CSVFeedSpider):
    name = 'csvdata'
    allowed_domains = ['gzdata.gov.cn']
    start_urls = ['http://gzopen.oss-cn-guizhou-a.aliyuncs.com/科技特派员.csv']
    headers = ['name', 'SearchField', 'Service', 'Specialty']
    delimiter = ','
    quotechar = &quot;\n&quot;

    # Do any adaptations you need here
    def adapt_response(self, response):
       return response.body.decode('gb18030')

    def parse_row(self, response, row):

        i = CsvspiderItem()
        try:
            i['name'] = row['name']
            i['SearchField'] = row['SearchField']
            i['Service'] = row['Service']
            i['Specialty'] = row['Specialty']

        except:
            pass
        yield i
</code></pre><p>步骤05: 运行爬虫文件</p><pre><code>scrapy crawl csvdata
</code></pre><h4 id=通用爬虫之sitemapspider>通用爬虫之SitemapSpider</h4><p>步骤01: 创建项目</p><pre><code>scrapy startproject cnblogs
</code></pre><p>步骤02: 编写items.py</p><pre><code># -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# https://doc.scrapy.org/en/latest/topics/items.html

import scrapy

class CnblogsItem(scrapy.Item):
    # define the fields for your item here like:
    # 文章标题
    title = scrapy.Field()
    # 文章url
    url = scrapy.Field()
    # 文章作者
    author = scrapy.Field()
</code></pre><p>步骤03: 在spiders文件夹内创建articles.py</p><pre><code>from scrapy.spiders import SitemapSpider
from cnblogs.items import CnblogsItem

class MySpider(SitemapSpider):
    name = 'articles'
    # Sitemap 地址
    sitemap_urls = ['http://www.cnblogs.com/sitemap.xml']
    # 从Sitemap中提取url的规则，并指定回调方法
    sitemap_rules = [
        # 抓取 ***/cate/python/**的url，调用parse_python处理
        ('/cate/python/','parse_python')
    ]

    # 回调方法
    def parse_python(self,response):
        articles = response.css('.post_item')

        for article in articles:
            item = CnblogsItem()
            # 文章标题
            item['title'] = article.css('.titlelnk::text').extract_first()
            # 文章url
            item['url'] = article.css('.titlelnk::attr(href)').extract_first()
            # 文章作者
            item['author'] = article.css('.lightblue::text').extract_first()
            yield item
</code></pre><p>步骤04: 运行爬虫</p><pre><code>scrapy crawl articles
</code></pre><h4 id=通用爬虫之xmlfeedspider>通用爬虫之XMLFeedSpider</h4><p>步骤01: 创建项目</p><pre><code>scrapy startproject xmlfeedspider
</code></pre><p>步骤02: 使用XMLFeedSpider模版创建爬虫</p><pre><code>scrapy genspider -t xmlfeed jobbole jobbole.com
</code></pre><p>步骤03: 修改items.py</p><pre><code>import scrapy

class JobboleItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    # 文章标题
    title = scrapy.Field()
    # 发表日期
    public_date = scrapy.Field()
    # 文章链接
    link = scrapy.Field()
</code></pre><p>步骤04: 配置爬虫文件jobbole.py</p><pre><code># -*- coding: utf-8 -*-
from scrapy.spiders import XMLFeedSpider
# 导入item
from xmlfeedspider.items import JobboleItem

class JobboleSpider(XMLFeedSpider):
    name = 'jobbole'
    allowed_domains = ['jobbole.com']
    start_urls = ['http://top.jobbole.com/feed/']
    iterator = 'iternodes'  # 迭代器，不指定的话默认是iternodes
    itertag = 'item'  # 抓取item节点

    def parse_node(self, response, selector):
        item = JobboleItem()
        item['title'] = selector.css('title::text').extract_first()
        item['public_date'] = selector.css('pubDate::text').extract_first()
        item['link'] = selector.css('link::text').extract_first()
        return item
</code></pre></div><div class=post-archive><ul class=post-copyright><li><strong>原文作者：</strong><a rel=author href=https://hank-leo.github.io>Hank</a></li><li style=word-break:break-all><strong>原文链接：</strong><a href=https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB/>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB/</a></li><li><strong>版权声明：</strong>本作品采用<a rel=license href=https://creativecommons.org/licenses/by-nc-nd/4.0/>知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议</a>进行许可，非商业转载请注明出处（作者，原文链接），商业转载请联系作者获得授权。</li></ul></div><br><div class=post-archive><h2>See Also</h2><ul class=listing><li><a href=/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Scrapy+MongoDB%E5%AE%9E%E6%88%98%E6%8A%93%E5%8F%96%E5%B9%B6%E4%BF%9D%E5%AD%98IT%E4%B9%8B%E5%AE%B6%E5%8D%9A%E5%AE%A2%E6%96%B0%E9%97%BB/>Scrapy框架:Scrapy+MongoDB实战:抓取并保存IT之家博客新闻</a></li><li><a href=/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/>Scrapy框架 基本命令</a></li><li><a href=/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%99%BB%E5%BD%95%E7%BD%91%E7%AB%99/>Scrapy框架 登录网站</a></li><li><a href=/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Request%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0/>Scrapy框架:Request回调函数</a></li><li><a href=/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-settings-py/>Scrapy框架:Settings.py</a></li></ul></div><div class="post-meta meta-tags"><ul class=clearfix><li><a href=https://hank-leo.github.io/tags/Scrapy>Scrapy</a></li></ul></div></article><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"yourdiscussshortname"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a><div class="post bg-white"><script src=https://utteranc.es/client.js repo=https://github.com/hank-leo/hank-leo.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div><footer id=footer><div>&copy; 2020 <a href=https://hank-leo.github.io>Hank's Blog By Hank</a></div><br><div><div class=github-badge><a href=https://gohugo.io/ target=_black rel=nofollow><span class=badge-subject>Powered by</span><span class="badge-value bg-blue">Hugo</span></a></div><div class=github-badge><a href=https://www.flysnow.org/ target=_black><span class=badge-subject>Design by</span><span class="badge-value bg-brightgreen">飞雪无情</span></a></div><div class=github-badge><a href=https://github.com/flysnow-org/maupassant-hugo target=_black><span class=badge-subject>Theme</span><span class="badge-value bg-yellowgreen">Maupassant</span></a></div></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src="/js/totop.js?v=0.0.0" async></script><script type=text/javascript src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async></script><script src=/js/douban.js></script></div><div id=secondary><section class=widget><form id=search action=https://hank-leo.github.io/search/ method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=Search>
<input type=hidden name=sitesearch value=https://hank-leo.github.io>
<button type=submit class="submit icon-search"></button></form></section><section class=widget><h3 class=widget-title>最近文章</h3><ul class=widget-list><li><a href=https://hank-leo.github.io/post/python/python%E7%BB%83%E4%B9%A0/ title=Python练习>Python练习</a></li><li><a href=https://hank-leo.github.io/post/python/Python%E9%9D%A2%E8%AF%95%E9%A2%98/ title=Python面试题>Python面试题</a></li><li><a href=https://hank-leo.github.io/post/spider/%E7%88%AC%E8%99%AB%E9%9D%A2%E8%AF%95%E9%A2%98/ title=爬虫面试题>爬虫面试题</a></li><li><a href=https://hank-leo.github.io/post/blog/%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/ title=使用hexo搭建个人博客>使用hexo搭建个人博客</a></li><li><a href=https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Scrapy+MongoDB%E5%AE%9E%E6%88%98%E6%8A%93%E5%8F%96%E5%B9%B6%E4%BF%9D%E5%AD%98IT%E4%B9%8B%E5%AE%B6%E5%8D%9A%E5%AE%A2%E6%96%B0%E9%97%BB/ title=Scrapy框架:Scrapy+MongoDB实战:抓取并保存IT之家博客新闻>Scrapy框架:Scrapy+MongoDB实战:抓取并保存IT之家博客新闻</a></li><li><a href=https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB/ title=Scrapy框架:通用爬虫>Scrapy框架:通用爬虫</a></li><li><a href=https://hank-leo.github.io/post/app/%E4%BD%BF%E7%94%A8Airtest%E8%BF%9B%E8%A1%8CApp%E7%88%AC%E8%99%AB/ title=使用Airtest进行App爬虫>使用Airtest进行App爬虫</a></li><li><a href=https://hank-leo.github.io/post/spider/%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E4%B9%8Bexcel/ title=存储数据之excel>存储数据之excel</a></li><li><a href=https://hank-leo.github.io/post/spider/%E8%A7%A3%E6%9E%90%E4%B8%8E%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B9%8Bre/ title=解析与提取数据之re>解析与提取数据之re</a></li><li><a href=https://hank-leo.github.io/post/spider/%E8%A7%A3%E6%9E%90%E4%B8%8E%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B9%8Bxpath/ title=解析与提取数据之xpath>解析与提取数据之xpath</a></li></ul></section><section class=widget><h3 class=widget-title><a href=/categories>分类</a></h3><ul class=widget-list><li><a href=https://hank-leo.github.io/categories/App%E7%88%AC%E8%99%AB/>App爬虫 (1)</a></li><li><a href=https://hank-leo.github.io/categories/Pandas%E5%AD%A6%E4%B9%A0/>Pandas学习 (3)</a></li><li><a href=https://hank-leo.github.io/categories/Python%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/>Python实际应用 (6)</a></li><li><a href=https://hank-leo.github.io/categories/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/>Scrapy框架学习 (11)</a></li><li><a href=https://hank-leo.github.io/categories/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/>博客搭建 (2)</a></li><li><a href=https://hank-leo.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%A6%E4%B9%A0/>数据库学习 (2)</a></li><li><a href=https://hank-leo.github.io/categories/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/>爬虫学习 (3)</a></li><li><a href=https://hank-leo.github.io/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/>面试题 (2)</a></li></ul></section><section class=widget><h3 class=widget-title><a href=/tags>标签</a></h3><div class=tagcloud><a href=https://hank-leo.github.io/tags/MongoDB/>MongoDB</a>
<a href=https://hank-leo.github.io/tags/MySQL/>MySQL</a>
<a href=https://hank-leo.github.io/tags/Scrapy/>Scrapy</a>
<a href=https://hank-leo.github.io/tags/airtest/>airtest</a>
<a href=https://hank-leo.github.io/tags/dict/>dict</a>
<a href=https://hank-leo.github.io/tags/excel/>excel</a>
<a href=https://hank-leo.github.io/tags/hexo/>hexo</a>
<a href=https://hank-leo.github.io/tags/hugo/>hugo</a>
<a href=https://hank-leo.github.io/tags/list/>list</a>
<a href=https://hank-leo.github.io/tags/pandas/>pandas</a>
<a href=https://hank-leo.github.io/tags/re/>re</a>
<a href=https://hank-leo.github.io/tags/regex/>regex</a>
<a href=https://hank-leo.github.io/tags/string/>string</a>
<a href=https://hank-leo.github.io/tags/xpath/>xpath</a>
<a href=https://hank-leo.github.io/tags/%E7%88%AC%E8%99%AB/>爬虫</a></div></section><section class=widget><h3 class=widget-title>友情链接</h3><ul class=widget-list><li><a target=_blank href=http://cnblogs.hankleo.com title=博客园>博客园</a></li></ul></section><section class=widget><h3 class=widget-title>其它</h3><ul class=widget-list><li><a href=https://hank-leo.github.io/index.xml>文章 RSS</a></li></ul></section></div></div></div></div></body></html>