<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scrapy on Hank's Blog</title><link>https://hank-leo.github.io/tags/Scrapy/</link><description>Recent content in Scrapy on Hank's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Fri, 06 Dec 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://hank-leo.github.io/tags/Scrapy/index.xml" rel="self" type="application/rss+xml"/><item><title>Scrapy框架:Scrapy+MongoDB实战:抓取并保存IT之家博客新闻</title><link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Scrapy+MongoDB%E5%AE%9E%E6%88%98%E6%8A%93%E5%8F%96%E5%B9%B6%E4%BF%9D%E5%AD%98IT%E4%B9%8B%E5%AE%B6%E5%8D%9A%E5%AE%A2%E6%96%B0%E9%97%BB/</link><pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate><guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Scrapy+MongoDB%E5%AE%9E%E6%88%98%E6%8A%93%E5%8F%96%E5%B9%B6%E4%BF%9D%E5%AD%98IT%E4%B9%8B%E5%AE%B6%E5%8D%9A%E5%AE%A2%E6%96%B0%E9%97%BB/</guid><description>创建项目 scrapy startproject ithome cd ithome scrapy genspider -t crawl news ithome.com 编写items.py文件 # -*- coding: utf-8 -*- # Define here the models for your scraped items # # See documentation in: # https://doc.scrapy.org/en/latest/topics/items.html import scrapy class IthomeItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() #文章标题 title = scrapy.Field() #文章url url = scrapy.Field() #来源 source = scrapy.Field() #来源url source_url = scrapy.Field() #发布日期 release_大特＝ scrapy.Field() #作者 author = scrapy.Field() #关键词 key_words = scrapy.Field() 编写爬虫文件 news.py # -*- coding: utf-8 -*- import</description></item><item><title>Scrapy框架:通用爬虫</title><link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB/</link><pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate><guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB/</guid><description>通用爬虫之CrawlSpider 步骤01: 创建爬虫项目 scrapy startproject quotes 步骤02: 创建爬虫模版 scrapy genspider -t quotes quotes.toscrape.com 步骤03: 配置爬虫文件quotes.py import scrapy from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors import LinkExtractor class Quotes(CrawlSpider): # 爬虫名称 name = &amp;quot;get_quotes&amp;quot; allow_domain = ['quotes.toscrape.com'] start_urls = ['http://quotes.toscrape.com/'] # 设定规则 rules = ( # 对于quotes内容页URL，调用parse_quotes处理， # 并以此规则</description></item><item><title>Scrapy框架 基本命令</title><link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/</link><pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate><guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/</guid><description>创建爬虫项目 scrapy startproject [项目名称] 创建爬虫文件 scrapy genspider +文件名+网址 运行(crawl) scrapy crawl 爬虫名称 # -o output 输出数据到文件 scrapy crawl [爬虫名称] -o zufang.json scrapy crawl [爬虫名称] -o zufang.csv check检查错误 scrapy check list返回项目所有spider scrapy list view 存储、打开网页 scrapy view http://www.baidu.com scrapy shell, 进入终端 scrapy shell https://www.baidu.com scrapy runspider scrapy runspider zufang_spider.py</description></item><item><title>Scrapy框架 登录网站</title><link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%99%BB%E5%BD%95%E7%BD%91%E7%AB%99/</link><pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate><guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%99%BB%E5%BD%95%E7%BD%91%E7%AB%99/</guid><description>使用cookies登录网站 import scrapy class LoginSpider(scrapy.Spider): name = 'login' allowed_domains = ['xxx.com'] start_urls = ['https://www.xxx.com/xx/'] cookies = &amp;quot;&amp;quot; def start_requests(self): for url in self.start_urls: yield scrapy.Request(url, cookies=self.cookies, callback=self.parse) def parse(self, response): with open(&amp;quot;01login.html&amp;quot;, &amp;quot;wb&amp;quot;) as f: f.write(response.body) 发送post请求登录, 要手动解析网页获取登录参数 import scrapy class LoginSpider(scrapy.Spider): name='login_code' allowed_domains = ['xxx.com'] #1. 登录页面 start_urls = ['https://www.xxx.com/login/'] def parse(self, response): #2. 代码登录 login_url='https://www.xxx.com/login' formdata={ &amp;quot;username&amp;quot;:&amp;quot;xxx&amp;quot;, &amp;quot;pwd&amp;quot;:&amp;quot;xxx&amp;quot;, &amp;quot;formhash&amp;quot;:response.xpath(&amp;quot;//input[@id='formhash']/@value&amp;quot;).extract_first(), &amp;quot;backurl&amp;quot;:response.xpath(&amp;quot;//input[@id='backurl']/@value&amp;quot;).extract_first() } #3. 发送登录请求post yield scrapy.FormRequest(login_url, formdata=formdata, callback=self.parse_login) def parse_login(self, response): #4.访问目标页面 member_url=&amp;quot;https://www.xxx.com/member&amp;quot; yield scrapy.Request(member_url, callback=self.parse_member) def parse_member(self, response): with open(&amp;quot;02login.html&amp;quot;,'wb') as</description></item><item><title>Scrapy框架:Request回调函数</title><link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Request%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0/</link><pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate><guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Request%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0/</guid><description>Request回调函数 def parse_page1(self, response): return scrapy.Request(&amp;quot;http://www.example.com/some_page.html&amp;quot;, callback=self.parse_page2) def parse_page2(self, response): # this would log http://www.example.com/some_page.html self.logger.info(&amp;quot;Visited %s&amp;quot;, response.url) 传递参数 def parse_page1(self, response): item = MyItem() item['name'] = response.css('.name::text').extract_first() request = scrapy.Request(&amp;quot;http://www.example.com/some_page.html&amp;quot;, callback=self.parse_page2) request.meta['item'] = item yield request def parse_page2(self, response): item = response.meta['item'] item['age'] = response.css('.age::text').extract_first() yield item</description></item><item><title>Scrapy框架:Settings.py</title><link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-settings-py/</link><pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate><guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-settings-py/</guid><description>#Scrapy项目名字 BOT_NAME = 'segmentfault' #Scrapy搜索spider的模块列表 SPIDER_MODULES = ['segmentfault.spiders'] #使用爬虫创建命令genspider创建爬虫时生成的模块 NEWSPIDER_MODULE = 'segmentfault.spiders' #默认的USER_AGENT, 使用BOT_NAME配置生成，建议覆盖 #USER_AGNET = 'segmentfault (+http://www.yourdomain.com)' #如果启用，Scrapy则会遵守网站Rebots.txt协议，建议设</description></item><item><title>Scrapy框架:入门案例</title><link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/</link><pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate><guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/</guid><description>创建项目: scrappy start project maitian 明确要抓取的字段items.py import scrapy class MaitianItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() title = scrapy.Field() price = scrapy.Field() area = scrapy.Field() district = scrapy.Field() 在spider目录下创建爬虫文件: zufang_spider.py 2.1 创建一个类，并继承scrapy的一个子类: scrapy.Spider 2.2 自定义爬取名, name=&amp;quot;&amp;quot; 后面运行框架需要用到； 2.3 定义爬取目标网址 2.4 定义scrapy的方法 下面是简</description></item><item><title>Scrapy框架:异常处理</title><link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/</link><pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate><guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/</guid><description>import scrapy from scrapy.spidermiddlewares.httperror import HttpError from twisted.internet.error import DNSLookupError from twisted.internet.error import TimeoutError, TCPTimedOutError class ErrbackSpider(scrapy.Spider): name = &amp;quot;errback_example&amp;quot; start_urls = [ &amp;quot;http://www.httpbin.org/&amp;quot;, # 正常HTTP 200返回 &amp;quot;http://www.httpbin.org/status/404&amp;quot;, # 404 Not found error &amp;quot;http://www.httpbin.org/status/500&amp;quot;, # 500服务器错误 &amp;quot;http://www.httpbin.org:12345/&amp;quot;, # 超时无响应错误 &amp;quot;http://www.httphttpbinbin.org/&amp;quot;, # DNS 错误 ] def start_requests(self): for u in self.start_urls: yield scrapy.Request(u, callback=self.parse_httpbin, errback=self.errback_httpbin, dont_filter=True) def parse_httpbin(self, response): self.logger.info('Got successful response from {}'.format(response.url)) # 其他处理. def errback_httpbin(self, failure): # 日志记录所有的异常信息 self.logger.error(repr(failure)) # 假设我们需要对指定的异常类型做处理， # 我们需要判断异常的类型 if</description></item><item><title>Scrapy框架:抓取猫眼电影TOP100</title><link>https://hank-leo.github.io/post/scrapy/scrapy%E6%A1%86%E6%9E%B6-%E6%8A%93%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1TOP100/</link><pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate><guid>https://hank-leo.github.io/post/scrapy/scrapy%E6%A1%86%E6%9E%B6-%E6%8A%93%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1TOP100/</guid><description>需求： 抓取的数据为电影名称、主演、上映日期、评分。将抓取的数据保存到maoyantop100.json文件，并将文件作为附件通过邮件发送给接收人。 创建项目 scrapy startproject maoyan scrapy genspider -t crawl top100 maoyan.com 编写items.py # -*- coding: utf-8 -*- import scrapy class MaoyanItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 电影名称 name = scrapy.Field() # 主演 actors = scrapy.Field() # 上映时间 releasetime = scrapy.Field()</description></item><item><title>Scrapy框架:统计数据收集</title><link>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/</link><pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate><guid>https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/</guid><description>通过stats属性来使用数据收集器 class ExtensionThatAccessStats(object): def __init__(self, stats): self.stats = stats @classmethod def from_crawler(cls, crawler): return cls(crawler.stats) 设置数据 stats.set_value('hostname', socket.gethostname()) 增加数据值 stats.inc_value('pages_crawled') 当新的值比原来的值大时设置数据 stats.max_value('max_items_scraoed', value) 当新的值比原来的值小时设置数据 stats.min_value('min_free_memory_percent', value) 获取数据 stats.get_value('pages_crawled') 获取所有数据 stats.get_stats() 示例：统计名人名言网站(http://quotes.toscrape.com/)标签为love的名言数</description></item></channel></rss>