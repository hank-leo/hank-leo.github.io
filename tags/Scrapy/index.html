<!doctype html>
<html lang="zh-CN">
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Scrapy | Hank&#39;s Blog</title>
    <meta property="og:title" content="Scrapy - Hank&#39;s Blog">
    <meta property="og:type" content="article">
        
        
    <meta name="Keywords" content="Hank,博客,python">
    <meta name="description" content="Scrapy">
        
    <meta name="author" content="Hank">
    <meta property="og:url" content="https://hank-leo.github.io/tags/Scrapy/">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

    <link rel="stylesheet" href='/css/normalize.css'>
    <link rel="stylesheet" href='/css/style.css'>
    <link rel="alternate" type="application/rss+xml+xml" href="https://hank-leo.github.io/tags/Scrapy/index.xml" title="Hank's Blog" />
    <script type="text/javascript" src="//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    
    
    
    
    
    
        <link rel="stylesheet" href='/css/douban.css'>
    
        <link rel="stylesheet" href='/css/other.css'>
    
</head>


<body>
    <header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <a id="logo" href="https://hank-leo.github.io">
                        Hank&#39;s Blog
                    </a>
                
                <p class="description">专注于Python、JavaScript、SQL、爬虫、数据分析与挖掘</p>
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="" href="https://hank-leo.github.io">首页</a>
                    
                    <a  href="https://hank-leo.github.io/archives/" title="归档">归档</a>
                    
                    <a  href="https://hank-leo.github.io/tags/" title="标签">标签</a>
                    
                    <a  href="https://hank-leo.github.io/categories/" title="分类">分类</a>
                    
                </nav>
            </div>
        </div>
    </div>
</header>

    <div id="body">
        <div class="container">
            <div class="col-group">

                <div class="col-8" id="main">
                    
<div class="res-cons">
    
    <h3 class="archive-title">
        包含标签
        <span class="keyword">Scrapy</span>
        的文章
    </h3>
    

    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Scrapy&#43;MongoDB%E5%AE%9E%E6%88%98%E6%8A%93%E5%8F%96%E5%B9%B6%E4%BF%9D%E5%AD%98IT%E4%B9%8B%E5%AE%B6%E5%8D%9A%E5%AE%A2%E6%96%B0%E9%97%BB/">Scrapy框架:Scrapy&#43;MongoDB实战:抓取并保存IT之家博客新闻</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2019年12月6日
        </date>
        
        <div class="post-meta meta-category">
            |
            
            <a href='https://hank-leo.github.io/categories/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0'>Scrapy框架学习</a>
            
        </div>
        
        <div class="post-content">
            创建项目 scrapy startproject ithome cd ithome scrapy genspider -t crawl news ithome.com 编写items.py文件 # -*- coding: utf-8 -*- # Define here the models for your scraped items # # See documentation in: # https://doc.scrapy.org/en/latest/topics/items.html import scrapy class IthomeItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() #文章标题 title = scrapy.Field() #文章url url = scrapy.Field() #来源 source = scrapy.Field() #来源url source_url = scrapy.Field() #发布日期 release_大特＝ scrapy.Field() #作者 author = scrapy.Field() #关键词 key_words = scrapy.Field() 编写爬虫文件 news.py # -*- coding: utf-8 -*- import……
            <p class="readmore"><a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Scrapy&#43;MongoDB%E5%AE%9E%E6%88%98%E6%8A%93%E5%8F%96%E5%B9%B6%E4%BF%9D%E5%AD%98IT%E4%B9%8B%E5%AE%B6%E5%8D%9A%E5%AE%A2%E6%96%B0%E9%97%BB/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB/">Scrapy框架:通用爬虫</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2019年12月6日
        </date>
        
        <div class="post-meta meta-category">
            |
            
            <a href='https://hank-leo.github.io/categories/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0'>Scrapy框架学习</a>
            
        </div>
        
        <div class="post-content">
            通用爬虫之CrawlSpider 步骤01: 创建爬虫项目 scrapy startproject quotes 步骤02: 创建爬虫模版 scrapy genspider -t quotes quotes.toscrape.com 步骤03: 配置爬虫文件quotes.py import scrapy from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors import LinkExtractor class Quotes(CrawlSpider): # 爬虫名称 name = &quot;get_quotes&quot; allow_domain = ['quotes.toscrape.com'] start_urls = ['http://quotes.toscrape.com/'] # 设定规则 rules = ( # 对于quotes内容页URL，调用parse_quotes处理， # 并以此规则……
            <p class="readmore"><a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/">Scrapy框架 基本命令</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2019年12月4日
        </date>
        
        <div class="post-meta meta-category">
            |
            
            <a href='https://hank-leo.github.io/categories/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0'>Scrapy框架学习</a>
            
        </div>
        
        <div class="post-content">
            创建爬虫项目 scrapy startproject [项目名称] 创建爬虫文件 scrapy genspider +文件名+网址 运行(crawl) scrapy crawl 爬虫名称 # -o output 输出数据到文件 scrapy crawl [爬虫名称] -o zufang.json scrapy crawl [爬虫名称] -o zufang.csv check检查错误 scrapy check list返回项目所有spider scrapy list view 存储、打开网页 scrapy view http://www.baidu.com scrapy shell, 进入终端 scrapy shell https://www.baidu.com scrapy runspider scrapy runspider zufang_spider.py……
            <p class="readmore"><a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%99%BB%E5%BD%95%E7%BD%91%E7%AB%99/">Scrapy框架 登录网站</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2019年12月4日
        </date>
        
        <div class="post-meta meta-category">
            |
            
            <a href='https://hank-leo.github.io/categories/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0'>Scrapy框架学习</a>
            
        </div>
        
        <div class="post-content">
            使用cookies登录网站 import scrapy class LoginSpider(scrapy.Spider): name = 'login' allowed_domains = ['xxx.com'] start_urls = ['https://www.xxx.com/xx/'] cookies = &quot;&quot; def start_requests(self): for url in self.start_urls: yield scrapy.Request(url, cookies=self.cookies, callback=self.parse) def parse(self, response): with open(&quot;01login.html&quot;, &quot;wb&quot;) as f: f.write(response.body) 发送post请求登录, 要手动解析网页获取登录参数 import scrapy class LoginSpider(scrapy.Spider): name='login_code' allowed_domains = ['xxx.com'] #1. 登录页面 start_urls = ['https://www.xxx.com/login/'] def parse(self, response): #2. 代码登录 login_url='https://www.xxx.com/login' formdata={ &quot;username&quot;:&quot;xxx&quot;, &quot;pwd&quot;:&quot;xxx&quot;, &quot;formhash&quot;:response.xpath(&quot;//input[@id='formhash']/@value&quot;).extract_first(), &quot;backurl&quot;:response.xpath(&quot;//input[@id='backurl']/@value&quot;).extract_first() } #3. 发送登录请求post yield scrapy.FormRequest(login_url, formdata=formdata, callback=self.parse_login) def parse_login(self, response): #4.访问目标页面 member_url=&quot;https://www.xxx.com/member&quot; yield scrapy.Request(member_url, callback=self.parse_member) def parse_member(self, response): with open(&quot;02login.html&quot;,'wb') as……
            <p class="readmore"><a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%99%BB%E5%BD%95%E7%BD%91%E7%AB%99/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Request%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0/">Scrapy框架:Request回调函数</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2019年12月4日
        </date>
        
        <div class="post-meta meta-category">
            |
            
            <a href='https://hank-leo.github.io/categories/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0'>Scrapy框架学习</a>
            
        </div>
        
        <div class="post-content">
            Request回调函数 def parse_page1(self, response): return scrapy.Request(&quot;http://www.example.com/some_page.html&quot;, callback=self.parse_page2) def parse_page2(self, response): # this would log http://www.example.com/some_page.html self.logger.info(&quot;Visited %s&quot;, response.url) 传递参数 def parse_page1(self, response): item = MyItem() item['name'] = response.css('.name::text').extract_first() request = scrapy.Request(&quot;http://www.example.com/some_page.html&quot;, callback=self.parse_page2) request.meta['item'] = item yield request def parse_page2(self, response): item = response.meta['item'] item['age'] = response.css('.age::text').extract_first() yield item……
            <p class="readmore"><a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Request%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-settings-py/">Scrapy框架:Settings.py</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2019年12月4日
        </date>
        
        <div class="post-meta meta-category">
            |
            
            <a href='https://hank-leo.github.io/categories/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0'>Scrapy框架学习</a>
            
        </div>
        
        <div class="post-content">
            #Scrapy项目名字 BOT_NAME = 'segmentfault' #Scrapy搜索spider的模块列表 SPIDER_MODULES = ['segmentfault.spiders'] #使用爬虫创建命令genspider创建爬虫时生成的模块 NEWSPIDER_MODULE = 'segmentfault.spiders' #默认的USER_AGENT, 使用BOT_NAME配置生成，建议覆盖 #USER_AGNET = 'segmentfault (+http://www.yourdomain.com)' #如果启用，Scrapy则会遵守网站Rebots.txt协议，建议设……
            <p class="readmore"><a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-settings-py/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/">Scrapy框架:入门案例</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2019年12月4日
        </date>
        
        <div class="post-meta meta-category">
            |
            
            <a href='https://hank-leo.github.io/categories/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0'>Scrapy框架学习</a>
            
        </div>
        
        <div class="post-content">
            创建项目: scrappy start project maitian 明确要抓取的字段items.py import scrapy class MaitianItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() title = scrapy.Field() price = scrapy.Field() area = scrapy.Field() district = scrapy.Field() 在spider目录下创建爬虫文件: zufang_spider.py 2.1 创建一个类，并继承scrapy的一个子类: scrapy.Spider 2.2 自定义爬取名, name=&quot;&quot; 后面运行框架需要用到； 2.3 定义爬取目标网址 2.4 定义scrapy的方法 下面是简……
            <p class="readmore"><a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/">Scrapy框架:异常处理</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2019年12月4日
        </date>
        
        <div class="post-meta meta-category">
            |
            
            <a href='https://hank-leo.github.io/categories/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0'>Scrapy框架学习</a>
            
        </div>
        
        <div class="post-content">
            import scrapy from scrapy.spidermiddlewares.httperror import HttpError from twisted.internet.error import DNSLookupError from twisted.internet.error import TimeoutError, TCPTimedOutError class ErrbackSpider(scrapy.Spider): name = &quot;errback_example&quot; start_urls = [ &quot;http://www.httpbin.org/&quot;, # 正常HTTP 200返回 &quot;http://www.httpbin.org/status/404&quot;, # 404 Not found error &quot;http://www.httpbin.org/status/500&quot;, # 500服务器错误 &quot;http://www.httpbin.org:12345/&quot;, # 超时无响应错误 &quot;http://www.httphttpbinbin.org/&quot;, # DNS 错误 ] def start_requests(self): for u in self.start_urls: yield scrapy.Request(u, callback=self.parse_httpbin, errback=self.errback_httpbin, dont_filter=True) def parse_httpbin(self, response): self.logger.info('Got successful response from {}'.format(response.url)) # 其他处理. def errback_httpbin(self, failure): # 日志记录所有的异常信息 self.logger.error(repr(failure)) # 假设我们需要对指定的异常类型做处理， # 我们需要判断异常的类型 if……
            <p class="readmore"><a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://hank-leo.github.io/post/scrapy/scrapy%E6%A1%86%E6%9E%B6-%E6%8A%93%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1TOP100/">Scrapy框架:抓取猫眼电影TOP100</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2019年12月4日
        </date>
        
        <div class="post-meta meta-category">
            |
            
            <a href='https://hank-leo.github.io/categories/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0'>Scrapy框架学习</a>
            
        </div>
        
        <div class="post-content">
            需求： 抓取的数据为电影名称、主演、上映日期、评分。将抓取的数据保存到maoyantop100.json文件，并将文件作为附件通过邮件发送给接收人。 创建项目 scrapy startproject maoyan scrapy genspider -t crawl top100 maoyan.com 编写items.py # -*- coding: utf-8 -*- import scrapy class MaoyanItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 电影名称 name = scrapy.Field() # 主演 actors = scrapy.Field() # 上映时间 releasetime = scrapy.Field()……
            <p class="readmore"><a href="https://hank-leo.github.io/post/scrapy/scrapy%E6%A1%86%E6%9E%B6-%E6%8A%93%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1TOP100/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/">Scrapy框架:统计数据收集</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2019年12月4日
        </date>
        
        <div class="post-meta meta-category">
            |
            
            <a href='https://hank-leo.github.io/categories/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0'>Scrapy框架学习</a>
            
        </div>
        
        <div class="post-content">
            通过stats属性来使用数据收集器 class ExtensionThatAccessStats(object): def __init__(self, stats): self.stats = stats @classmethod def from_crawler(cls, crawler): return cls(crawler.stats) 设置数据 stats.set_value('hostname', socket.gethostname()) 增加数据值 stats.inc_value('pages_crawled') 当新的值比原来的值大时设置数据 stats.max_value('max_items_scraoed', value) 当新的值比原来的值小时设置数据 stats.min_value('min_free_memory_percent', value) 获取数据 stats.get_value('pages_crawled') 获取所有数据 stats.get_stats() 示例：统计名人名言网站(http://quotes.toscrape.com/)标签为love的名言数……
            <p class="readmore"><a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/">阅读全文</a></p>
        </div>
    </article>
    

    





</div>

                    <footer id="footer">
    <div>
        &copy; 2020 <a href="https://hank-leo.github.io">Hank&#39;s Blog By Hank</a>
        
    </div>
    <br />
    <div>
        <div class="github-badge">
            <a href="https://gohugo.io/" target="_black" rel="nofollow"><span class="badge-subject">Powered by</span><span class="badge-value bg-blue">Hugo</span></a>
        </div>
        <div class="github-badge">
            <a href="https://www.flysnow.org/" target="_black"><span class="badge-subject">Design by</span><span class="badge-value bg-brightgreen">飞雪无情</span></a>
        </div>
        <div class="github-badge">
            <a href="https://github.com/flysnow-org/maupassant-hugo" target="_black"><span class="badge-subject">Theme</span><span class="badge-value bg-yellowgreen">Maupassant</span></a>
        </div>
    </div>
</footer>



<a id="rocket" href="#top"></a>
<script type="text/javascript" src='/js/totop.js?v=0.0.0' async=""></script>



    <script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>




    <script src='/js/douban.js'></script>

                </div>

                <div id="secondary">
    <section class="widget">
        <form id="search" action='https://hank-leo.github.io/search/' method="get" accept-charset="utf-8" target="_blank" _lpchecked="1">
      
      <input type="text" name="q" maxlength="20" placeholder="Search">
      <input type="hidden" name="sitesearch" value="https://hank-leo.github.io">
      <button type="submit" class="submit icon-search"></button>
</form>
    </section>
    
    <section class="widget">
        <h3 class="widget-title">最近文章</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://hank-leo.github.io/post/blog/%E4%BD%BF%E7%94%A8Github-Actions%E6%90%AD%E5%BB%BAhugo/" title="使用Github Actions搭建hugo">使用Github Actions搭建hugo</a>
    </li>
    
    <li>
        <a href="https://hank-leo.github.io/post/python/python%E7%BB%83%E4%B9%A0/" title="Python练习">Python练习</a>
    </li>
    
    <li>
        <a href="https://hank-leo.github.io/post/python/Python%E9%9D%A2%E8%AF%95%E9%A2%98/" title="Python面试题">Python面试题</a>
    </li>
    
    <li>
        <a href="https://hank-leo.github.io/post/spider/%E7%88%AC%E8%99%AB%E9%9D%A2%E8%AF%95%E9%A2%98/" title="爬虫面试题">爬虫面试题</a>
    </li>
    
    <li>
        <a href="https://hank-leo.github.io/post/blog/%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" title="使用hexo搭建个人博客">使用hexo搭建个人博客</a>
    </li>
    
    <li>
        <a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6-Scrapy&#43;MongoDB%E5%AE%9E%E6%88%98%E6%8A%93%E5%8F%96%E5%B9%B6%E4%BF%9D%E5%AD%98IT%E4%B9%8B%E5%AE%B6%E5%8D%9A%E5%AE%A2%E6%96%B0%E9%97%BB/" title="Scrapy框架:Scrapy&#43;MongoDB实战:抓取并保存IT之家博客新闻">Scrapy框架:Scrapy&#43;MongoDB实战:抓取并保存IT之家博客新闻</a>
    </li>
    
    <li>
        <a href="https://hank-leo.github.io/post/scrapy/Scrapy%E6%A1%86%E6%9E%B6%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB/" title="Scrapy框架:通用爬虫">Scrapy框架:通用爬虫</a>
    </li>
    
    <li>
        <a href="https://hank-leo.github.io/post/app/%E4%BD%BF%E7%94%A8Airtest%E8%BF%9B%E8%A1%8CApp%E7%88%AC%E8%99%AB/" title="使用Airtest进行App爬虫">使用Airtest进行App爬虫</a>
    </li>
    
    <li>
        <a href="https://hank-leo.github.io/post/spider/%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E4%B9%8Bexcel/" title="存储数据之excel">存储数据之excel</a>
    </li>
    
    <li>
        <a href="https://hank-leo.github.io/post/spider/%E8%A7%A3%E6%9E%90%E4%B8%8E%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B9%8Bre/" title="解析与提取数据之re">解析与提取数据之re</a>
    </li>
    
</ul>
    </section>

    

    <section class="widget">
        <h3 class="widget-title"><a href="/categories">分类</a></h3>
<ul class="widget-list">
    
    <li><a href="https://hank-leo.github.io/categories/App%E7%88%AC%E8%99%AB/">App爬虫 (1)</a></li>
    
    <li><a href="https://hank-leo.github.io/categories/Pandas%E5%AD%A6%E4%B9%A0/">Pandas学习 (3)</a></li>
    
    <li><a href="https://hank-leo.github.io/categories/Python%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/">Python实际应用 (6)</a></li>
    
    <li><a href="https://hank-leo.github.io/categories/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/">Scrapy框架学习 (11)</a></li>
    
    <li><a href="https://hank-leo.github.io/categories/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/">博客搭建 (3)</a></li>
    
    <li><a href="https://hank-leo.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%A6%E4%B9%A0/">数据库学习 (2)</a></li>
    
    <li><a href="https://hank-leo.github.io/categories/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/">爬虫学习 (3)</a></li>
    
    <li><a href="https://hank-leo.github.io/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/">面试题 (2)</a></li>
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title"><a href="/tags">标签</a></h3>
<div class="tagcloud">
    
    <a href="https://hank-leo.github.io/tags/MongoDB/">MongoDB</a>
    
    <a href="https://hank-leo.github.io/tags/MySQL/">MySQL</a>
    
    <a href="https://hank-leo.github.io/tags/Scrapy/">Scrapy</a>
    
    <a href="https://hank-leo.github.io/tags/airtest/">airtest</a>
    
    <a href="https://hank-leo.github.io/tags/dict/">dict</a>
    
    <a href="https://hank-leo.github.io/tags/excel/">excel</a>
    
    <a href="https://hank-leo.github.io/tags/hexo/">hexo</a>
    
    <a href="https://hank-leo.github.io/tags/hugo/">hugo</a>
    
    <a href="https://hank-leo.github.io/tags/list/">list</a>
    
    <a href="https://hank-leo.github.io/tags/pandas/">pandas</a>
    
    <a href="https://hank-leo.github.io/tags/re/">re</a>
    
    <a href="https://hank-leo.github.io/tags/regex/">regex</a>
    
    <a href="https://hank-leo.github.io/tags/string/">string</a>
    
    <a href="https://hank-leo.github.io/tags/xpath/">xpath</a>
    
    <a href="https://hank-leo.github.io/tags/%E7%88%AC%E8%99%AB/">爬虫</a>
    
</div>
    </section>

    
<section class="widget">
    <h3 class="widget-title">友情链接</h3>
    <ul class="widget-list">
        
        <li>
            <a target="_blank" href="http://cnblogs.hankleo.com" title="博客园">博客园</a>
        </li>
        
    </ul>
</section>


    <section class="widget">
        <h3 class="widget-title">其它</h3>
        <ul class="widget-list">
            <li><a href="https://hank-leo.github.io/index.xml">文章 RSS</a></li>
        </ul>
    </section>
</div>
            </div>
        </div>
    </div>
</body>

</html>